{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud PoC â€” Robust LLM ingestion, parsing and repair\n",
    "\n",
    "This notebook contains integrated, ready-to-run cells to:\n",
    "- Backup the DuckDB file\n",
    "- Provide robust streaming assembly for Ollama responses\n",
    "- Parse numeric `risk_score` reliably\n",
    "- Insert per-transaction LLM results (one LLM call per tx)\n",
    "- Reprocess rows with missing/NaN `risk_score` (repair)\n",
    "\n",
    "Update DB_PATH below if your DB file is located elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a179611d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB_PATH = fraud_poc.duckdb\n",
      "OLLAMA_URL = http://localhost:11434/api/generate\n",
      "MODEL = olmo-3\n"
     ]
    }
   ],
   "source": [
    "# CONFIG\n",
    "import os\n",
    "DB_PATH = os.environ.get('FRAUD_DB_PATH', 'fraud_poc.duckdb')   # change if needed\n",
    "OLLAMA_URL = os.environ.get('OLLAMA_URL', 'http://localhost:11434/api/generate')\n",
    "MODEL = os.environ.get('LLM_MODEL', 'olmo-3')\n",
    "print('DB_PATH =', DB_PATH)\n",
    "print('OLLAMA_URL =', OLLAMA_URL)\n",
    "print('MODEL =', MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f1d3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created: fraud_poc.duckdb.bak\n"
     ]
    }
   ],
   "source": [
    "# Backup the DB file (run in notebook)\n",
    "import shutil\n",
    "if os.path.exists(DB_PATH):\n",
    "    bak = DB_PATH + '.bak'\n",
    "    shutil.copy2(DB_PATH, bak)\n",
    "    print(f'Backup created: {bak}')\n",
    "else:\n",
    "    raise FileNotFoundError(f'DB not found at {DB_PATH}; set DB_PATH correctly and run this cell again.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb02c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and robust parsing helpers\n",
    "import duckdb, json, re, math, datetime, uuid, requests\n",
    "import numpy as np\n",
    "\n",
    "def parse_risk_score(value):\n",
    "    \"\"\"Return float in 0..1 or math.nan if not parseable.\"\"\"\n",
    "    if value is None:\n",
    "        return math.nan\n",
    "    # numeric types\n",
    "    if isinstance(value, (int, float, np.integer, np.floating)):\n",
    "        v = float(value)\n",
    "        return math.nan if math.isnan(v) else v\n",
    "    s = str(value).strip()\n",
    "    # try JSON content\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict):\n",
    "            # common keys\n",
    "            for key in (\"risk_score\",\"score\",\"risk\",\"riskScore\"):\n",
    "                if key in obj:\n",
    "                    return parse_risk_score(obj[key])\n",
    "        elif isinstance(obj, (int, float)):\n",
    "            return float(obj)\n",
    "    except Exception:\n",
    "        pass\n",
    "    low = s.lower()\n",
    "    if low in (\"\",\"null\",\"none\",\"n/a\",\"na\",\"nan\"):\n",
    "        return math.nan\n",
    "    # percent like 82%\n",
    "    m = re.search(r'(-?\\d+(?:[.,]\\d+)?)\\s*%', s)\n",
    "    if m:\n",
    "        try:\n",
    "            num = float(m.group(1).replace(',','.'))\n",
    "            return num/100.0\n",
    "        except:\n",
    "            return math.nan\n",
    "    # find first numeric token\n",
    "    m = re.search(r'(-?\\d+(?:[.,]\\d+)?)', s)\n",
    "    if m:\n",
    "        try:\n",
    "            num = float(m.group(1).replace(',','.'))\n",
    "        except:\n",
    "            return math.nan\n",
    "        if num < 0:\n",
    "            return math.nan\n",
    "        if num > 1 and num <= 100:\n",
    "            return num/100.0\n",
    "        return float(num)\n",
    "    return math.nan\n",
    "\n",
    "def extract_final_text_from_response(raw):\n",
    "    \"\"\"Attempt to get final textual output from a streaming llm_response field.\n",
    "    If raw contains newline-separated JSON lines, parse last JSON that has 'response' or numeric keys.\n",
    "    Otherwise return the last non-empty line or entire text as fallback.\n",
    "    \"\"\"\n",
    "    if raw is None:\n",
    "        return \"\"\n",
    "    text = str(raw)\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    # search from last line backwards\n",
    "    for ln in reversed(lines):\n",
    "        try:\n",
    "            obj = json.loads(ln)\n",
    "            if isinstance(obj, dict):\n",
    "                # direct numeric key\n",
    "                for key in (\"risk_score\",\"score\",\"risk\",\"riskScore\"):\n",
    "                    if key in obj:\n",
    "                        return obj[key]\n",
    "                if obj.get('response'):\n",
    "                    return obj['response']\n",
    "                if obj.get('thinking'):\n",
    "                    return obj['thinking']\n",
    "            elif isinstance(obj, (int,float)):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            # not JSON; consider this line as candidate\n",
    "            if len(ln) > 0:\n",
    "                return ln\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d2e52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming wrapper to call Ollama and assemble final text (per prompt)\n",
    "def call_ollama_stream(prompt, model=MODEL, ollama_url=OLLAMA_URL, timeout=300):\n",
    "    payload = {\"model\": model, \"prompt\": prompt, \"temperature\": 0.0, \"max_tokens\": 512}\n",
    "    resp = requests.post(ollama_url, json=payload, stream=True, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    assembled = \"\"\n",
    "    raw_lines = []\n",
    "    for line in resp.iter_lines(decode_unicode=True):\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            chunk = json.loads(line)\n",
    "            raw_lines.append(chunk)\n",
    "        except Exception:\n",
    "            raw_lines.append({'text': line})\n",
    "            continue\n",
    "        if chunk.get('response'):\n",
    "            assembled += chunk['response']\n",
    "        elif chunk.get('thinking'):\n",
    "            assembled += chunk['thinking']\n",
    "        if chunk.get('done'):\n",
    "            break\n",
    "    return assembled, raw_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b06a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB insert helper that stores parsed_response, llm_response, raw_lines, and flags needs_review\n",
    "def safe_insert_llm_result(con, row_id, tx_id, model, assembled, raw_lines, parsed_val, needs_review, now):\n",
    "    parsed_json = {\"parsed_risk\": None if math.isnan(parsed_val) else float(parsed_val)}\n",
    "    # Ensure needs_review column exists; add if missing\n",
    "    try:\n",
    "        con.execute(\"ALTER TABLE llm_results ADD COLUMN IF NOT EXISTS needs_review BOOLEAN DEFAULT FALSE\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    con.execute(\"\"\"\n",
    "        INSERT INTO llm_results (id, tx_id, llm_model, llm_response, parsed_response, risk_score, needs_review, created_at)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        row_id,\n",
    "        tx_id,\n",
    "        model,\n",
    "        assembled,\n",
    "        json.dumps(parsed_json),\n",
    "        (None if math.isnan(parsed_val) else float(parsed_val)),\n",
    "        needs_review,\n",
    "        now\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08a110f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 unprocessed txs (limit 10).\n",
      "Inserted: tx_id=tx_000011 id=77715f6b-ae5a-4783-8734-e2216bdc1e87 risk_score=-0.15 needs_review=True\n",
      "Inserted: tx_id=tx_000012 id=5a340584-1dcf-45e1-ae5e-47a26191dd8c risk_score=0.0 needs_review=False\n",
      "Inserted: tx_id=tx_000013 id=176b33d0-5b22-4a7c-bc3f-bc6f0b0dccbf risk_score=0.0 needs_review=False\n",
      "Inserted: tx_id=tx_000014 id=37f03f1b-840a-4b09-9415-d98ea251156e risk_score=0.3 needs_review=False\n",
      "Inserted: tx_id=tx_000015 id=96569138-3ebd-4909-9a68-4c444984a079 risk_score=0.0 needs_review=False\n",
      "Inserted: tx_id=tx_000016 id=f1f0792d-8653-4018-b210-29201423c886 risk_score=-0.2 needs_review=True\n",
      "Inserted: tx_id=tx_000017 id=a440783a-16c7-402f-b310-c99b1825146c risk_score=0.0 needs_review=False\n",
      "Inserted: tx_id=tx_000018 id=0f566012-09e0-46a7-9e89-cab62a17a0dc risk_score=0.41 needs_review=False\n",
      "Inserted: tx_id=tx_000019 id=0c411dda-d190-45b2-8859-f81d1f42e62c risk_score=0.0 needs_review=False\n",
      "Inserted: tx_id=tx_000020 id=78795e71-4675-41cb-a3cb-2497c338f185 risk_score=0.5 needs_review=False\n"
     ]
    }
   ],
   "source": [
    "# Example: process unprocessed transactions (one LLM call per tx)\n",
    "con = duckdb.connect(DB_PATH)\n",
    "unprocessed_txs = con.execute(\"\"\"\n",
    "    SELECT t.tx_id, t.account_id, t.amount, t.currency, t.merchant, t.description\n",
    "    FROM transactions t\n",
    "    LEFT JOIN llm_results l ON t.tx_id = l.tx_id\n",
    "    WHERE l.id IS NULL\n",
    "    LIMIT 10\n",
    "\"\"\").fetchall()\n",
    "print(f\"Found {len(unprocessed_txs)} unprocessed txs (limit 10).\")\n",
    "for tx in unprocessed_txs:\n",
    "    tx_id, account_id, amount, currency, merchant, description = tx\n",
    "    prompt = f\"Transaction: account={account_id} amount={amount} {currency} merchant={merchant} description={description}\\n\\nReturn a numeric risk_score between 0 and 1 and a short explanation.\" \n",
    "    try:\n",
    "        assembled, raw_lines = call_ollama_stream(prompt)\n",
    "    except Exception as exc:\n",
    "        print(f\"LLM call failed for tx {tx_id}: {exc}\")\n",
    "        # insert placeholder row marked for review\n",
    "        row_id = str(uuid.uuid4())\n",
    "        now = datetime.datetime.utcnow()\n",
    "        safe_insert_llm_result(con, row_id, tx_id, MODEL, \"\", [{\"error\": str(exc)}], math.nan, True, now)\n",
    "        continue\n",
    "    parsed_val = parse_risk_score(assembled)\n",
    "    needs_review = False\n",
    "    if math.isnan(parsed_val) or parsed_val < 0 or parsed_val > 1:\n",
    "        needs_review = True\n",
    "    else:\n",
    "        parsed_val = max(0.0, min(1.0, float(parsed_val)))\n",
    "    row_id = str(uuid.uuid4())\n",
    "    now = datetime.datetime.utcnow()\n",
    "    safe_insert_llm_result(con, row_id, tx_id, MODEL, assembled, raw_lines, parsed_val, needs_review, now)\n",
    "    print(f\"Inserted: tx_id={tx_id} id={row_id} risk_score={parsed_val} needs_review={needs_review}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57957467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows to reprocess: 3\n",
      "Reprocessed ea39f16f-25f5-414a-9f94-5e362757bee3: risk_score=1.0 needs_review=False\n",
      "Reprocessed 93b0a2e5-8a25-4a9c-b59b-0fd6cbe9d9ce: risk_score=0.7 needs_review=False\n",
      "Reprocessed 65f50de2-c08e-409a-bdff-a7e0f1b50c63: risk_score=0.0 needs_review=False\n"
     ]
    }
   ],
   "source": [
    "# Repair: reprocess rows with NULL/NaN risk_score if you have tx data available\n",
    "con = duckdb.connect(DB_PATH)\n",
    "to_reprocess = con.execute(\"\"\"\n",
    "SELECT l.id, l.tx_id, t.account_id, t.amount, t.currency, t.merchant, t.description\n",
    "FROM llm_results l\n",
    "LEFT JOIN transactions t ON l.tx_id = t.tx_id\n",
    "WHERE l.risk_score IS NULL OR (l.risk_score != l.risk_score)\n",
    "LIMIT 100\n",
    "\"\"\").fetchall()\n",
    "print(f\"Rows to reprocess: {len(to_reprocess)}\")\n",
    "for row in to_reprocess:\n",
    "    llm_id, tx_id, account_id, amount, currency, merchant, description = row\n",
    "    if tx_id is None:\n",
    "        print(f\"No tx data for llm result {llm_id}; skipping\")\n",
    "        continue\n",
    "    prompt = f\"Transaction: account={account_id} amount={amount} {currency} merchant={merchant} description={description}\\n\\nReturn a numeric risk_score between 0 and 1 and a short explanation.\" \n",
    "    try:\n",
    "        assembled, raw_lines = call_ollama_stream(prompt)\n",
    "    except Exception as exc:\n",
    "        print(f\"Reprocess failed for llm row {llm_id}: {exc}\")\n",
    "        continue\n",
    "    parsed_val = parse_risk_score(assembled)\n",
    "    needs_review = False\n",
    "    if math.isnan(parsed_val) or parsed_val < 0 or parsed_val > 1:\n",
    "        needs_review = True\n",
    "    else:\n",
    "        parsed_val = max(0.0, min(1.0, float(parsed_val)))\n",
    "    # update existing row\n",
    "    con.execute(\"\"\"\n",
    "        UPDATE llm_results\n",
    "        SET llm_response = ?, parsed_response = ?, risk_score = ?, needs_review = ?\n",
    "        WHERE id = ?\n",
    "    \"\"\", (assembled, json.dumps({\"parsed_risk\": None if math.isnan(parsed_val) else parsed_val}), (None if math.isnan(parsed_val) else parsed_val), needs_review, llm_id))\n",
    "    print(f\"Reprocessed {llm_id}: risk_score={parsed_val} needs_review={needs_review}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c36c11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total llm_results rows: 23\n",
      "Missing/NaN risk_score count: 0\n",
      "Needs review count: 2\n",
      "\n",
      "Sample needs_review rows:\n",
      "('f1f0792d-8653-4018-b210-29201423c886', 'tx_000016', -0.2, '{\"parsed_risk\": -0.2}', 'Okay, so I need to figure out a risk score between 0 and 1 for this transaction. Let me start by looking at the details given. The account is acct_0024, and the amount is -48.94 USD. The merchant is o')\n",
      "('77715f6b-ae5a-4783-8734-e2216bdc1e87', 'tx_000011', -0.15, '{\"parsed_risk\": -0.15}', 'Okay, so I need to figure out a risk score between 0 and 1 for this transaction. Let me start by looking at the details provided. The transaction is from account acct_0038 with an amount of $113.88 US')\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics: show remaining missing / flagged rows\n",
    "con = duckdb.connect(DB_PATH)\n",
    "print('Total llm_results rows:', con.execute('SELECT COUNT(*) FROM llm_results').fetchone()[0])\n",
    "print('Missing/NaN risk_score count:', con.execute(\"SELECT COUNT(*) FROM llm_results WHERE risk_score IS NULL OR (risk_score != risk_score)\").fetchone()[0])\n",
    "print('Needs review count:', con.execute(\"SELECT COUNT(*) FROM llm_results WHERE needs_review = TRUE\").fetchone()[0])\n",
    "print('\\nSample needs_review rows:')\n",
    "rows = con.execute(\"SELECT id, tx_id, risk_score, parsed_response, SUBSTR(llm_response,1,200) FROM llm_results WHERE needs_review = TRUE ORDER BY created_at DESC LIMIT 10\").fetchall()\n",
    "for r in rows:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e1dae",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- The notebook assumes your `llm_results` table has columns: id, tx_id, llm_model, llm_response, parsed_response, risk_score, needs_review, created_at. If your schema differs, adjust the SQL and column names accordingly.\n",
    "- The `call_ollama_stream` function uses streaming to assemble text. It must be used once per transaction (or use batch prompts if you prefer).\n",
    "- After running this notebook, you should see NaNs reduced. Any rows where parsing still fails will be marked needs_review.\n",
    "If you want, I can also produce a PR that replaces the notebook in your repository with this version. Tell me whether you want a PR (and which target branch), or whether you'd prefer to paste these cells into your existing notebook yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
