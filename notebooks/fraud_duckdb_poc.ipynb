{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5340ad74",
   "metadata": {},
   "source": [
    "# Fraud PoC — robust & efficient LLM ingestion (batching, parallelism, retries)\n",
    "\n",
    "This updated notebook includes comprehensive options to process transactions with an LLM while minimizing latency and handling timeouts:\n",
    "- Modes: `batch` (recommended), `parallel`, `sequential`\n",
    "- Adaptive batching with exponential backoff and fallback to smaller batches or per-item processing\n",
    "- Non-streaming and streaming call implementations with retries\n",
    "- Automatic model preloading and fallback to smaller models if resources are constrained\n",
    "- Bulk inserts to DuckDB for efficiency\n",
    "- Repair flow for missing/NaN risk scores\n",
    "\n",
    "Usage: set environment variables if you want to override defaults:\n",
    "- `FRAUD_DB_PATH` — path to duckdb file (default `notebooks/fraud_poc.duckdb`)\n",
    "- `OLLAMA_URL` — default `http://localhost:11434/api/generate`\n",
    "- `LLM_MODEL` — preferred model (default `olmo-3`)\n",
    "- `PROCESS_MODE` — one of `batch`, `parallel`, `sequential`, or `auto` (default `auto`)\n",
    "\n",
    "Run cells top-to-bottom; adjust `CONFIG` cell parameters to tune behavior for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de75f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host CPU cores: 12\n",
      "Host memory GB (approx): 83.47371673583984\n",
      "ollama available in PATH? False\n",
      "preferred model present (cli check): False\n"
     ]
    }
   ],
   "source": [
    "# Basic environment checks and helper functions (memory/cpu and model availability)\n",
    "import subprocess, shutil, sys, time\n",
    "import json, re\n",
    "\n",
    "def get_total_memory_gb():\n",
    "    # Linux: read /proc/meminfo, fallback to shutil.disk_usage not ideal for mem\n",
    "    try:\n",
    "        with open('/proc/meminfo') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('MemTotal:'):\n",
    "                    parts = line.split()\n",
    "                    # value is in kB\n",
    "                    kb = int(parts[1])\n",
    "                    return kb / 1024.0 / 1024.0\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def get_cpu_count():\n",
    "    try:\n",
    "        return os.cpu_count() or 1\n",
    "    except Exception:\n",
    "        return 1\n",
    "\n",
    "def is_model_present(model_name):\n",
    "    # Use ollama CLI if available to check installed models; fallback to returning False\n",
    "    try:\n",
    "        out = subprocess.run(['ollama', 'list'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False, text=True)\n",
    "        return model_name in out.stdout\n",
    "    except FileNotFoundError:\n",
    "        # CLI not present — can't check; assume server may have model\n",
    "        return False\n",
    "\n",
    "def pull_model_if_missing(model_name):\n",
    "    try:\n",
    "        print('Attempting ollama pull', model_name)\n",
    "        subprocess.run(['ollama', 'pull', model_name], check=True)\n",
    "        print('Model pull completed')\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print('ollama CLI not in PATH; skipping pull')\n",
    "        return False\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print('ollama pull failed:', e)\n",
    "        return False\n",
    "\n",
    "print('Host CPU cores:', get_cpu_count())\n",
    "print('Host memory GB (approx):', get_total_memory_gb())\n",
    "print('ollama available in PATH?', shutil.which('ollama') is not None)\n",
    "print('preferred model present (cli check):', is_model_present(PREFERRED_MODEL))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe5e8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call helpers ready\n"
     ]
    }
   ],
   "source": [
    "# Robust call helpers: non-streaming and streaming with retries\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "def requests_session_with_retries(total_retries=3, backoff=1.0):\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=total_retries, backoff_factor=backoff, status_forcelist=[429,500,502,503,504])\n",
    "    s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "    s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    return s\n",
    "\n",
    "def call_ollama_non_stream(prompt, model=PREFERRED_MODEL, ollama_url=OLLAMA_URL, timeout=300, session=None):\n",
    "    payload = {\"model\": model, \"prompt\": prompt, \"temperature\": 0.0, \"max_tokens\": 512}\n",
    "    sess = session or requests_session_with_retries()\n",
    "    r = sess.post(ollama_url, json=payload, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    # Try parse JSON; if not, return text\n",
    "    try:\n",
    "        return r.json(), r.text\n",
    "    except Exception:\n",
    "        return None, r.text\n",
    "\n",
    "def call_ollama_stream(prompt, model=PREFERRED_MODEL, ollama_url=OLLAMA_URL, timeout=600):\n",
    "    # streaming with long timeout and basic retries managed by caller\n",
    "    payload = {\"model\": model, \"prompt\": prompt, \"temperature\": 0.0, \"max_tokens\": 512}\n",
    "    r = requests.post(ollama_url, json=payload, stream=True, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    assembled = ''\n",
    "    raw_lines = []\n",
    "    for line in r.iter_lines(decode_unicode=True):\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            chunk = json.loads(line)\n",
    "            raw_lines.append(chunk)\n",
    "        except Exception:\n",
    "            raw_lines.append({'text': line})\n",
    "            continue\n",
    "        if chunk.get('response'):\n",
    "            assembled += chunk['response']\n",
    "        elif chunk.get('thinking'):\n",
    "            assembled += chunk['thinking']\n",
    "        if chunk.get('done'):\n",
    "            break\n",
    "    return assembled, raw_lines\n",
    "\n",
    "print('Call helpers ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fcc195b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing helpers ready\n"
     ]
    }
   ],
   "source": [
    "# Parsing helpers (same robust parser and extractor)\n",
    "import numpy as np\n",
    "def parse_risk_score(value):\n",
    "    import math, json, re\n",
    "    if value is None:\n",
    "        return math.nan\n",
    "    if isinstance(value, (int, float, np.integer, np.floating)):\n",
    "        v = float(value)\n",
    "        return math.nan if math.isnan(v) else v\n",
    "    s = str(value).strip()\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict):\n",
    "            for key in (\"risk_score\",\"score\",\"risk\",\"riskScore\"):\n",
    "                if key in obj:\n",
    "                    return parse_risk_score(obj[key])\n",
    "        elif isinstance(obj, (int, float)):\n",
    "            return float(obj)\n",
    "    except Exception:\n",
    "        pass\n",
    "    low = s.lower()\n",
    "    if low in (\"\",\"null\",\"none\",\"n/a\",\"na\",\"nan\"):\n",
    "        return math.nan\n",
    "    m = re.search(r'(-?\\d+(?:[.,]\\d+)?)\\s*%', s)\n",
    "    if m:\n",
    "        try:\n",
    "            num = float(m.group(1).replace(',','.'))\n",
    "            return num/100.0\n",
    "        except:\n",
    "            return math.nan\n",
    "    m = re.search(r'(-?\\d+(?:[.,]\\d+)?)', s)\n",
    "    if m:\n",
    "        try:\n",
    "            num = float(m.group(1).replace(',','.'))\n",
    "        except:\n",
    "            return math.nan\n",
    "        if num < 0:\n",
    "            return math.nan\n",
    "        if num > 1 and num <= 100:\n",
    "            return num/100.0\n",
    "        return float(num)\n",
    "    return math.nan\n",
    "\n",
    "def extract_final_text_from_response(raw):\n",
    "    if raw is None:\n",
    "        return ''\n",
    "    text = str(raw)\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    for ln in reversed(lines):\n",
    "        try:\n",
    "            obj = json.loads(ln)\n",
    "            if isinstance(obj, dict):\n",
    "                for key in (\"risk_score\",\"score\",\"risk\",\"riskScore\"):\n",
    "                    if key in obj:\n",
    "                        return obj[key]\n",
    "                if obj.get('response'):\n",
    "                    return obj['response']\n",
    "                if obj.get('thinking'):\n",
    "                    return obj['thinking']\n",
    "            elif isinstance(obj, (int,float)):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            if len(ln) > 0:\n",
    "                return ln\n",
    "    return text\n",
    "\n",
    "print('Parsing helpers ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44afc139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulk insert helper ready\n"
     ]
    }
   ],
   "source": [
    "# Bulk insert helper\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "def bulk_insert_llm_results(con, results):\n",
    "    if len(results) == 0:\n",
    "        return 0\n",
    "    df = pd.DataFrame(results)\n",
    "    # ensure needs_review column exists\n",
    "    try:\n",
    "        con.execute('ALTER TABLE llm_results ADD COLUMN IF NOT EXISTS needs_review BOOLEAN DEFAULT FALSE')\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Insert using a registered dataframe\n",
    "    con.register('batch_df', df)\n",
    "    cols = ', '.join(df.columns)\n",
    "    con.execute(f'INSERT INTO llm_results ({cols}) SELECT {cols} FROM batch_df')\n",
    "    return len(results)\n",
    "\n",
    "print('Bulk insert helper ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70bd91db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive batch runner defined\n"
     ]
    }
   ],
   "source": [
    "# Adaptive batching runner with retries, fallback to smaller batch sizes and per-item processing\n",
    "from math import ceil\n",
    "import datetime\n",
    "\n",
    "def process_batches_and_insert(DB_PATH, mode='batch', batch_size=BATCH_SIZE, min_batch=MIN_BATCH_SIZE, max_timeout=MAX_BATCH_CALL_TIMEOUT):\n",
    "    print(con.execute(\"SHOW TABLES\").fetchall())\n",
    "    rows = con.execute(\"\"\"\n",
    "    SELECT t.tx_id, t.account_id, t.amount, t.currency, t.merchant, t.description\n",
    "    FROM transactions t\n",
    "    LEFT JOIN llm_results l ON t.tx_id = l.tx_id\n",
    "    WHERE l.id IS NULL\n",
    "    LIMIT 1000\n",
    "    \"\"\").df()\n",
    "    print('Fetched', len(rows), 'unprocessed txs')\n",
    "    results_to_insert = []\n",
    "    sess = requests_session_with_retries(total_retries=2, backoff=1)\n",
    "\n",
    "    def attempt_batch(prompt, timeout):\n",
    "        # try non-streaming first (may be faster to get full JSON array)\n",
    "        try:\n",
    "            parsed_json, text = call_ollama_non_stream(prompt, timeout=timeout, session=sess)\n",
    "            if isinstance(parsed_json, (list, dict)):\n",
    "                return parsed_json, text, None\n",
    "            # parsed_json may be None but text contains content\n",
    "            return None, text, None\n",
    "        except Exception as e:\n",
    "            # fallback to streaming call\n",
    "            try:\n",
    "                assembled, raw_lines = call_ollama_stream(prompt, timeout=timeout)\n",
    "                return None, assembled, raw_lines\n",
    "            except Exception as e2:\n",
    "                return None, None, e2\n",
    "\n",
    "    # helper to build strict JSON batch prompt\n",
    "    def build_batch_prompt(tx_records):\n",
    "        prompt = (\n",
    "            'You are an assistant that evaluates transaction fraud risk.\\n'\n",
    "            'For each transaction below, return a JSON array where each element is an object: '\n",
    "            '{\"tx_id\": <tx_id>, \"risk_score\": <number between 0 and 1>, \"explanation\": <short string>}\\n'\n",
    "            'Return ONLY the JSON array — no extra text.\\n\\nTransactions:\\n'\n",
    "        )\n",
    "        for r in tx_records:\n",
    "            prompt += f\"- tx_id={r['tx_id']} account={r['account_id']} amount={r['amount']} {r['currency']} merchant={r['merchant']} description={r['description']}\\n\"\n",
    "        prompt += '\\nRespond with a JSON array matching the order of the transactions.'\n",
    "        return prompt\n",
    "\n",
    "    i = 0\n",
    "    while i < len(rows):\n",
    "        current_batch = rows.iloc[i:i+batch_size].to_dict(orient='records')\n",
    "        prompt = build_batch_prompt(current_batch)\n",
    "        timeout = min(max_timeout, max(60, 60 * ceil(len(current_batch)/2)))\n",
    "        print(f'Calling batch starting at index {i}, size {len(current_batch)}, timeout {timeout}s')\n",
    "        parsed, text_or_assembled, raw_or_err = attempt_batch(prompt, timeout)\n",
    "        if parsed is None and isinstance(text_or_assembled, str) and raw_or_err is None:\n",
    "            # we have text (non-json) or assembled stream\n",
    "            assembled_text = text_or_assembled\n",
    "            # attempt to parse JSON from assembled_text\n",
    "            try:\n",
    "                parsed_try = json.loads(assembled_text)\n",
    "                if not isinstance(parsed_try, list):\n",
    "                    parsed_try = None\n",
    "            except Exception:\n",
    "                parsed_try = None\n",
    "            if parsed_try is None:\n",
    "                # If batch_size > min_batch, retry with half-size\n",
    "                if batch_size > min_batch and len(current_batch) > min_batch:\n",
    "                    print('Batch JSON parse failed — reducing batch size and retrying')\n",
    "                    # split the current batch into two half batches by adjusting batch_size locally\n",
    "                    # we'll requeue the second half by inserting it back at position i+half\n",
    "                    half = max(min_batch, len(current_batch)//2)\n",
    "                    # schedule second half next\n",
    "                    # do not increment i by full batch_size; instead process first half now\n",
    "                    # process first half immediately by setting local_batch and prompt\n",
    "                    first_half = current_batch[:half]\n",
    "                    second_half = current_batch[half:]\n",
    "                    # process first_half now by setting rows slice accordingly\n",
    "                    rows = pd.concat([rows.iloc[:i], pd.DataFrame(first_half), pd.DataFrame(second_half), rows.iloc[i+len(current_batch):]]).reset_index(drop=True)\n",
    "                    # keep batch_size same and continue (the inserted first_half will be processed next loop)\n",
    "                    continue\n",
    "                else:\n",
    "                    # cannot split further — fallback to per-item processing for this batch\n",
    "                    print('Falling back to per-item processing for this batch')\n",
    "                    for rec in current_batch:\n",
    "                        single_prompt = build_batch_prompt([rec])\n",
    "                        try:\n",
    "                            parsed_obj, text = call_ollama_non_stream(single_prompt, timeout=SEQUENTIAL_TIMEOUT, session=sess)\n",
    "                            if isinstance(parsed_obj, list) and len(parsed_obj) == 1:\n",
    "                                entry = parsed_obj[0]\n",
    "                                raw_score = entry.get('risk_score') or entry.get('score')\n",
    "                                parsed_val = parse_risk_score(raw_score)\n",
    "                            else:\n",
    "                                assembled_single, raw_lines_single = call_ollama_stream(single_prompt, timeout=SEQUENTIAL_TIMEOUT)\n",
    "                                extracted = extract_final_text_from_response(assembled_single)\n",
    "                                parsed_val = parse_risk_score(extracted)\n",
    "                        except Exception as ee:\n",
    "                            parsed_val = None\n",
    "                        needs_review = parsed_val is None or (isinstance(parsed_val, float) and (parsed_val != parsed_val))\n",
    "                        results_to_insert.append({\n",
    "                            'id': str(uuid.uuid4()), 'tx_id': rec['tx_id'], 'llm_model': PREFERRED_MODEL,\n",
    "                            'llm_response': assembled_single if 'assembled_single' in locals() else '',\n",
    "                            'parsed_response': json.dumps({'parsed_risk': None if parsed_val is None else parsed_val}),\n",
    "                            'risk_score': (None if parsed_val is None else float(parsed_val)), 'needs_review': needs_review, 'created_at': datetime.datetime.utcnow()\n",
    "                        })\n",
    "                    i += len(current_batch)\n",
    "                    continue\n",
    "            else:\n",
    "                parsed = parsed_try\n",
    "                # proceed to accept parsed\n",
    "        elif parsed is not None:\n",
    "            # parsed is returned directly (non-streaming best case)\n",
    "            pass\n",
    "        elif isinstance(raw_or_err, Exception) or text_or_assembled is None:\n",
    "            # request failed even after streaming attempt — mark all for review\n",
    "            print('Batch call failed even after streaming; marking batch for review:', raw_or_err)\n",
    "            for rec in current_batch:\n",
    "                results_to_insert.append({\n",
    "                    'id': str(uuid.uuid4()), 'tx_id': rec['tx_id'], 'llm_model': PREFERRED_MODEL,\n",
    "                    'llm_response': '' , 'parsed_response': json.dumps({'error': str(raw_or_err)}), 'risk_score': None, 'needs_review': True, 'created_at': datetime.datetime.utcnow()\n",
    "                })\n",
    "            i += len(current_batch)\n",
    "            continue\n",
    "\n",
    "        # If we have a parsed list, map entries\n",
    "        if parsed is None and isinstance(parsed_try, list):\n",
    "            parsed = parsed_try\n",
    "\n",
    "        if isinstance(parsed, list):\n",
    "            if len(parsed) != len(current_batch):\n",
    "                # mismatch — mark for review\n",
    "                print('Length mismatch between parsed JSON and batch; marking batch for review')\n",
    "                for rec in current_batch:\n",
    "                    results_to_insert.append({\n",
    "                        'id': str(uuid.uuid4()), 'tx_id': rec['tx_id'], 'llm_model': PREFERRED_MODEL,\n",
    "                        'llm_response': (text_or_assembled if isinstance(text_or_assembled, str) else json.dumps(text_or_assembled)),\n",
    "                        'parsed_response': json.dumps(parsed), 'risk_score': None, 'needs_review': True, 'created_at': datetime.datetime.utcnow()\n",
    "                    })\n",
    "            else:\n",
    "                for entry in parsed:\n",
    "                    tx_id = entry.get('tx_id')\n",
    "                    raw_score = entry.get('risk_score') if 'risk_score' in entry else entry.get('score') if 'score' in entry else None\n",
    "                    parsed_val = parse_risk_score(raw_score)\n",
    "                    needs_review = math.isnan(parsed_val) or parsed_val < 0 or parsed_val > 1\n",
    "                    results_to_insert.append({\n",
    "                        'id': str(uuid.uuid4()),\n",
    "                        'tx_id': tx_id,\n",
    "                        'llm_model': PREFERRED_MODEL,\n",
    "                        'llm_response': (text_or_assembled if isinstance(text_or_assembled, str) else json.dumps(text_or_assembled)),\n",
    "                        'parsed_response': json.dumps(entry),\n",
    "                        'risk_score': (None if needs_review else float(parsed_val)),\n",
    "                        'needs_review': needs_review,\n",
    "                        'created_at': datetime.datetime.utcnow()\n",
    "                    })\n",
    "        i += len(current_batch)\n",
    "    # Bulk insert all results\n",
    "    inserted = bulk_insert_llm_results(con, results_to_insert)\n",
    "    print('Inserted', inserted, 'rows')\n",
    "    return inserted\n",
    "\n",
    "print('Adaptive batch runner defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef850c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel runner defined\n"
     ]
    }
   ],
   "source": [
    "# Parallel per-tx runner (safe small concurrency)\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "def process_parallel_and_insert(db_path, workers=PARALLEL_WORKERS, max_items=200):\n",
    "    ##con = duckdb.connect(db_path)\n",
    "    df = con.execute(\"SELECT t.tx_id, t.account_id, t.amount, t.currency, t.merchant, t.description FROM transactions t LEFT JOIN llm_results l ON t.tx_id = l.tx_id WHERE l.id IS NULL LIMIT ?\", [max_items]).df()\n",
    "    tx_list = df.to_dict(orient='records')\n",
    "    print('Parallel runner: txs to process =', len(tx_list))\n",
    "    results = []\n",
    "    def worker(tx):\n",
    "        prompt = f\"Transaction: account={tx['account_id']} amount={tx['amount']} {tx['currency']} merchant={tx['merchant']} description={tx['description']}\\n\\nReturn a JSON object with tx_id and risk_score between 0 and 1 and a short explanation.\"\n",
    "        try:\n",
    "            parsed_obj, text = call_ollama_non_stream(prompt, timeout=SEQUENTIAL_TIMEOUT)\n",
    "            if isinstance(parsed_obj, dict):\n",
    "                raw_score = parsed_obj.get('risk_score') or parsed_obj.get('score')\n",
    "            else:\n",
    "                assembled, raw_lines = call_ollama_stream(prompt, timeout=SEQUENTIAL_TIMEOUT)\n",
    "                raw_score = extract_final_text_from_response(assembled)\n",
    "        except Exception as e:\n",
    "            return {'id': str(uuid.uuid4()), 'tx_id': tx['tx_id'], 'llm_model': PREFERRED_MODEL, 'llm_response': '', 'parsed_response': json.dumps({'error': str(e)}), 'risk_score': None, 'needs_review': True, 'created_at': datetime.datetime.utcnow()}\n",
    "        parsed_val = parse_risk_score(raw_score)\n",
    "        needs_review = math.isnan(parsed_val) or parsed_val < 0 or parsed_val > 1\n",
    "        parsed_db = None if needs_review else float(parsed_val)\n",
    "        return {'id': str(uuid.uuid4()), 'tx_id': tx['tx_id'], 'llm_model': PREFERRED_MODEL, 'llm_response': (assembled if 'assembled' in locals() else (text if 'text' in locals() else '')), 'parsed_response': json.dumps({'parsed_risk': parsed_db}), 'risk_score': parsed_db, 'needs_review': needs_review, 'created_at': datetime.datetime.utcnow()}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as exe:\n",
    "        futures = {exe.submit(worker, tx): tx for tx in tx_list}\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                results.append(fut.result())\n",
    "            except Exception as e:\n",
    "                print('Worker error', e)\n",
    "    inserted = bulk_insert_llm_results(con, results)\n",
    "    print('Parallel inserted', inserted, 'rows')\n",
    "    return inserted\n",
    "\n",
    "print('Parallel runner defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3685dc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator selected mode: batch\n",
      "[]\n"
     ]
    },
    {
     "ename": "CatalogException",
     "evalue": "Catalog Error: Table with name transactions does not exist!\nDid you mean \"duckdb_constraints\"?\n\nLINE 3:     FROM transactions t\n                 ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatalogException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1045257773.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconservative_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0minserted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_batches_and_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDB_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mconservative_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'parallel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0minserted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_parallel_and_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDB_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPARALLEL_WORKERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3869644983.py\u001b[0m in \u001b[0;36mprocess_batches_and_insert\u001b[0;34m(DB_PATH, mode, batch_size, min_batch, max_timeout)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_batches_and_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDB_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMIN_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_BATCH_CALL_TIMEOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SHOW TABLES\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     rows = con.execute(\"\"\"\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mSELECT\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtx_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccount_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerchant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mFROM\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCatalogException\u001b[0m: Catalog Error: Table with name transactions does not exist!\nDid you mean \"duckdb_constraints\"?\n\nLINE 3:     FROM transactions t\n                 ^"
     ]
    }
   ],
   "source": [
    "# Orchestrator: pick mode and run with safety checks\n",
    "conservative_mode = PROCESS_MODE\n",
    "if PROCESS_MODE == 'auto':\n",
    "    # Heuristic: if machine memory < 8GB, prefer sequential or small batch and smaller model\n",
    "    mem = get_total_memory_gb() or 0\n",
    "    if mem and mem < 8:\n",
    "        conservative_mode = 'sequential'\n",
    "    else:\n",
    "        conservative_mode = 'batch'\n",
    "\n",
    "print('Orchestrator selected mode:', conservative_mode)\n",
    "\n",
    "if conservative_mode == 'batch':\n",
    "    inserted = process_batches_and_insert(DB_PATH, mode='batch', batch_size=BATCH_SIZE)\n",
    "elif conservative_mode == 'parallel':\n",
    "    inserted = process_parallel_and_insert(DB_PATH, workers=PARALLEL_WORKERS)\n",
    "else:\n",
    "    # sequential fallback: same as batch with min_batch=1\n",
    "    inserted = process_batches_and_insert(DB_PATH, mode='sequential', batch_size=1, min_batch=1, max_timeout=SEQUENTIAL_TIMEOUT)\n",
    "print('Done — total rows inserted (approx):', inserted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2dec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repair: reprocess rows with NULL/NaN risk_score using the same logic (attempt batch then per-item)\n",
    "def repair_missing(db_path, limit=200):\n",
    "    con = duckdb.connect(db_path)\n",
    "    to_reprocess = con.execute(\"\"\"\n",
    "    SELECT l.id as llm_id, l.tx_id, t.account_id, t.amount, t.currency, t.merchant, t.description\n",
    "    FROM llm_results l\n",
    "    LEFT JOIN transactions t ON l.tx_id = t.tx_id\n",
    "    WHERE l.risk_score IS NULL OR (l.risk_score != l.risk_score)\n",
    "    LIMIT ?\n",
    "    \"\"\", [limit]).df()\n",
    "    print('Repair candidates:', len(to_reprocess))\n",
    "    # reuse batch runner but process each row individually (safe)\n",
    "    for row in to_reprocess.to_dict(orient='records'):\n",
    "        llm_id = row['llm_id']\n",
    "        tx_id = row['tx_id']\n",
    "        if tx_id is None:\n",
    "            continue\n",
    "        prompt = f\"Transaction: account={row['account_id']} amount={row['amount']} {row['currency']} merchant={row['merchant']} description={row['description']}\\n\\nReturn a JSON object with tx_id and risk_score between 0 and 1 and a short explanation.\"\n",
    "        try:\n",
    "            parsed_obj, text = call_ollama_non_stream(prompt, timeout=SEQUENTIAL_TIMEOUT)\n",
    "            if isinstance(parsed_obj, dict):\n",
    "                raw_score = parsed_obj.get('risk_score') or parsed_obj.get('score')\n",
    "            else:\n",
    "                assembled, raw_lines = call_ollama_stream(prompt, timeout=SEQUENTIAL_TIMEOUT)\n",
    "                raw_score = extract_final_text_from_response(assembled)\n",
    "        except Exception as e:\n",
    "            print('Repair call failed for', llm_id, e)\n",
    "            continue\n",
    "        parsed_val = parse_risk_score(raw_score)\n",
    "        needs_review = math.isnan(parsed_val) or parsed_val < 0 or parsed_val > 1\n",
    "        parsed_db = None if needs_review else float(max(0.0, min(1.0, parsed_val)))\n",
    "        con.execute(\"\"\"\n",
    "            UPDATE llm_results\n",
    "            SET llm_response = ?, parsed_response = ?, risk_score = ?, needs_review = ?\n",
    "            WHERE id = ?\n",
    "        \"\"\", ( (assembled if 'assembled' in locals() else (text if 'text' in locals() else '')), json.dumps({'parsed_risk': parsed_db}), parsed_db, needs_review, llm_id))\n",
    "        print('Repaired', llm_id, '->', parsed_db, 'needs_review=', needs_review)\n",
    "    print('Repair pass complete')\n",
    "\n",
    "print('Repair function ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final diagnostics\n",
    "con = duckdb.connect(DB_PATH)\n",
    "print('Total transactions:', con.execute('SELECT COUNT(*) FROM transactions').fetchone()[0])\n",
    "print('Total llm_results rows:', con.execute('SELECT COUNT(*) FROM llm_results').fetchone()[0])\n",
    "print('Missing/NaN risk_score count:', con.execute(\"SELECT COUNT(*) FROM llm_results WHERE risk_score IS NULL OR (risk_score != risk_score)\").fetchone()[0])\n",
    "print('Needs review count:', con.execute(\"SELECT COUNT(*) FROM llm_results WHERE needs_review = TRUE\").fetchone()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f477b27",
   "metadata": {},
   "source": [
    "Notes, tips and next steps:\n",
    "- The notebook implements three approaches: batching (recommended), parallel small-concurrency workers, and sequential per-item fallback. It adapts to failures by reducing batch size and ultimately doing per-item reprocessing.\n",
    "- If you continue to see long batch durations or timeouts, try:\n",
    "  - Decreasing BATCH_SIZE to 2–4\n",
    "  - Using a smaller model (set LLM_MODEL env var to `phi-2.7b` or `gemma:2b`)\n",
    "  - Increasing timeouts for slow CPU inference hosts (MAX_BATCH_CALL_TIMEOUT)\n",
    "  - Running fewer parallel workers or using sequential mode on low-memory Codespaces\n",
    "- The orchestrator picks `batch` when host memory appears adequate; if memory is low it uses `sequential`. You can override with PROCESS_MODE.\n",
    "- If you'd like, I can commit this notebook to your repository on a branch and open a PR. Tell me which branch to base from and the PR branch name if you'd like that done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
