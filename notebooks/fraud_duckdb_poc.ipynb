{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python (fraud-poc)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fraud PoC â€” DuckDB + Embeddings + Ollama\n",
        "\n",
        "This notebook demonstrates an end-to-end PoC:\n",
        "- create canonical tables in DuckDB\n",
        "- ingest synthetic transactions\n",
        "- compute embeddings (sentence-transformers)\n",
        "- build a local ANN index (faiss / hnswlib / sklearn fallback)\n",
        "- perform retrieval, assemble prompt and call Ollama (local LLM)\n",
        "- persist LLM result and provenance back into DuckDB\n",
        "\n",
        "Notes:\n",
        "- Ensure your environment has the dependencies from `requirements.txt` installed.\n",
        "- Ensure Ollama is running and reachable at OLLAMA_URL (default http://localhost:11434/api/generate).\n",
        "- The notebook auto-falls back to hnswlib or sklearn NearestNeighbors if faiss is unavailable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "# Configuration & imports\n",
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import time\n",
        "import hashlib\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Embedding lib\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ANN fallbacks\n",
        "HAS_FAISS = False\n",
        "try:\n",
        "    import faiss\n",
        "    HAS_FAISS = True\n",
        "except Exception:\n",
        "    HAS_FAISS = False\n",
        "\n",
        "HAS_HNSW = False\n",
        "try:\n",
        "    import hnswlib\n",
        "    HAS_HNSW = True\n",
        "except Exception:\n",
        "    HAS_HNSW = False\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Ollama endpoint (adjust if different in your setup)\n",
        "OLLAMA_URL = os.environ.get('OLLAMA_URL', 'http://localhost:11434/api/generate')\n",
        "OLLAMA_MODEL = os.environ.get('OLLAMA_MODEL', 'llama2')  # replace with your pulled model name\n",
        "\n",
        "# DuckDB path\n",
        "DB_PATH = os.environ.get('DB_PATH', 'fraud_poc.duckdb')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "# Utilities: prompt hashing and calling Ollama\n",
        "def prompt_hash(prompt_text: str) -> str:\n",
        "    return hashlib.sha256(prompt_text.encode('utf-8')).hexdigest()\n",
        "\n",
        "def call_ollama(model: str, prompt_text: str, temperature: float = 0.0, timeout: int = 60) -> Dict[str, Any]:\n",
        "    payload = {\n",
        "        'model': model,\n",
        "        'prompt': prompt_text,\n",
        "        'temperature': temperature\n",
        "    }\n",
        "    start = time.time()\n",
        "    resp = requests.post(OLLAMA_URL, json=payload, timeout=timeout)\n",
        "    resp.raise_for_status()\n",
        "    elapsed_ms = int((time.time() - start) * 1000)\n",
        "    try:\n",
        "        data = resp.json()\n",
        "    except ValueError:\n",
        "        data = {'text': resp.text}\n",
        "    # Normalize text extraction (common keys)\n",
        "    text = data.get('text') or data.get('response') or data.get('content') or resp.text\n",
        "    return {\n",
        "        'llm_provider': 'ollama_local',\n",
        "        'llm_model': model,\n",
        "        'llm_response_raw': text,\n",
        "        'llm_response_json': data if isinstance(data, dict) else None,\n",
        "        'latency_ms': elapsed_ms,\n",
        "        'prompt_hash': prompt_hash(prompt_text),\n",
        "        'call_id': str(uuid.uuid4())\n",
        "    }\n",
        "\n",
        "def try_parse_json(s: str):\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "# Create DuckDB connection and required tables (idempotent)\n",
        "con = duckdb.connect(DB_PATH)\n",
        "\n",
        "CREATE_TRANSACTIONS = '''\n",
        "CREATE TABLE IF NOT EXISTS transactions (\n",
        "  tx_id VARCHAR PRIMARY KEY,\n",
        "  account_id VARCHAR,\n",
        "  amount DOUBLE,\n",
        "  currency VARCHAR,\n",
        "  merchant VARCHAR,\n",
        "  description VARCHAR,\n",
        "  timestamp TIMESTAMP,\n",
        "  ingestion_job_id VARCHAR,\n",
        "  raw_source VARCHAR,\n",
        "  pii_masked BOOLEAN DEFAULT FALSE,\n",
        "  created_at TIMESTAMP DEFAULT current_timestamp\n",
        ");\n",
        "'''\n",
        "\n",
        "CREATE_EMBEDDINGS = '''\n",
        "CREATE TABLE IF NOT EXISTS embeddings (\n",
        "  tx_id VARCHAR PRIMARY KEY,\n",
        "  emb_json TEXT,\n",
        "  emb_model VARCHAR,\n",
        "  emb_created_at TIMESTAMP DEFAULT current_timestamp,\n",
        "  emb_job_id VARCHAR\n",
        ");\n",
        "'''\n",
        "\n",
        "CREATE_LLM = '''\n",
        "CREATE TABLE IF NOT EXISTS llm_results (\n",
        "  id VARCHAR PRIMARY KEY,\n",
        "  tx_id VARCHAR,\n",
        "  llm_model VARCHAR,\n",
        "  llm_provider VARCHAR,\n",
        "  llm_prompt_hash VARCHAR,\n",
        "  llm_prompt VARCHAR,\n",
        "  llm_response VARCHAR,\n",
        "  parsed_response JSON,\n",
        "  risk_score DOUBLE,\n",
        "  evidence_tx_ids JSON,\n",
        "  call_latency_ms INTEGER,\n",
        "  provenance JSON,\n",
        "  created_at TIMESTAMP DEFAULT current_timestamp\n",
        ");\n",
        "'''\n",
        "\n",
        "con.execute(CREATE_TRANSACTIONS)\n",
        "con.execute(CREATE_EMBEDDINGS)\n",
        "con.execute(CREATE_LLM)\n",
        "print('Created tables (if not existing) in', DB_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "# Create synthetic transactions and insert into DuckDB\n",
        "import datetime\n",
        "\n",
        "n = 200\n",
        "rng = np.random.default_rng(42)\n",
        "txs = []\n",
        "for i in range(n):\n",
        "    tx_id = f\"tx_{i+1:06d}\"\n",
        "    account = f\"acct_{rng.integers(1,50):04d}\"\n",
        "    amount = float(np.round(rng.normal(50, 120), 2))\n",
        "    merchant = rng.choice(['store_a','store_b','online_shop','gas_station','restaurant'])\n",
        "    desc = f\"purchase at {merchant} for ${amount:.2f}\"\n",
        "    ts = datetime.datetime.utcnow() - datetime.timedelta(minutes=int(rng.integers(0, 60*24)))\n",
        "    txs.append((tx_id, account, amount, 'USD', merchant, desc, ts, 'ingest_0', 'synthetic', False))\n",
        "\n",
        "df = pd.DataFrame(txs, columns=['tx_id','account_id','amount','currency','merchant','description','timestamp','ingestion_job_id','raw_source','pii_masked'])\n",
        "\n",
        "con.register('df_tx', df)\n",
        "con.execute(\"INSERT INTO transactions SELECT * FROM df_tx\")\n",
        "print('Inserted', len(df), 'transactions')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "# Compute embeddings with sentence-transformers and store in embeddings table\n",
        "embed_model_name = 'all-MiniLM-L6-v2'\n",
        "print('Loading embedding model:', embed_model_name)\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "# Load transactions into a DataFrame (from DuckDB)\n",
        "df_tx = con.execute('SELECT tx_id, description FROM transactions').df()\n",
        "texts = df_tx['description'].tolist()\n",
        "vectors = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "import json\n",
        "rows = []\n",
        "for tx_id, vec in zip(df_tx['tx_id'].tolist(), vectors):\n",
        "    rows.append((tx_id, json.dumps(vec.tolist()), embed_model_name, None))\n",
        "\n",
        "df_emb = pd.DataFrame(rows, columns=['tx_id','emb_json','emb_model','emb_job_id'])\n",
        "con.register('df_emb', df_emb)\n",
        "con.execute(\"INSERT INTO embeddings SELECT * FROM df_emb\")\n",
        "print('Stored', len(df_emb), 'embeddings in DuckDB')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "# Build an in-memory ANN index (faiss preferred, then hnswlib, then sklearn)\n",
        "emb_matrix = np.vstack([np.array(json.loads(x)).astype('float32') for x in con.execute('SELECT emb_json FROM embeddings ORDER BY emb_created_at').df()['emb_json']])\n",
        "ids = con.execute('SELECT tx_id FROM embeddings ORDER BY emb_created_at').df()['tx_id'].tolist()\n",
        "id_to_index = {tx_id: idx for idx, tx_id in enumerate(ids)}\n",
        "\n",
        "ann_index = None\n",
        "ann_backend = None\n",
        "dim = emb_matrix.shape[1]\n",
        "\n",
        "if HAS_FAISS:\n",
        "    import faiss\n",
        "    # normalize for cosine similarity\n",
        "    xb = emb_matrix.copy()\n",
        "    faiss.normalize_L2(xb)\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(xb)\n",
        "    ann_index = index\n",
        "    ann_backend = 'faiss'\n",
        "    print('Built FAISS IndexFlatIP with', xb.shape[0], 'vectors')\n",
        "elif HAS_HNSW:\n",
        "    import hnswlib\n",
        "    p = hnswlib.Index(space='cosine', dim=dim)\n",
        "    p.init_index(max_elements=emb_matrix.shape[0], ef_construction=200, M=16)\n",
        "    p.add_items(emb_matrix, np.arange(emb_matrix.shape[0]))\n",
        "    p.set_ef(50)\n",
        "    ann_index = p\n",
        "    ann_backend = 'hnswlib'\n",
        "    print('Built hnswlib index with', emb_matrix.shape[0], 'vectors')\n",
        "else:\n",
        "    # sklearn brute-force as fallback\n",
        "    nbrs = NearestNeighbors(n_neighbors=10, metric='cosine', algorithm='brute').fit(emb_matrix)\n",
        "    ann_index = nbrs\n",
        "    ann_backend = 'sklearn'\n",
        "    print('Built sklearn NearestNeighbors fallback (brute)')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "# Retrieval helper\n",
        "def retrieve_topk(query_vec: np.ndarray, k: int = 5) -> List[Tuple[str, float]]:\n",
        "    q = query_vec.astype('float32')\n",
        "    if ann_backend == 'faiss':\n",
        "        qn = q.copy()\n",
        "        faiss.normalize_L2(qn)\n",
        "        D, I = ann_index.search(qn.reshape(1, -1), k)\n",
        "        # FAISS IndexFlatIP returns inner product; since we normalized, it's cosine similarity\n",
        "        return [(ids[int(i)], float(D[0][idx])) for idx, i in enumerate(I[0])] \n",
        "    elif ann_backend == 'hnswlib':\n",
        "        labels, distances = ann_index.knn_query(q, k=k)\n",
        "        return [(ids[int(lbl)], float(dist)) for lbl, dist in zip(labels[0], distances[0])]\n",
        "    else:\n",
        "        D, I = ann_index.kneighbors(q.reshape(1, -1), n_neighbors=k, return_distance=True)\n",
        "        return [(ids[int(i)], float(D[0][idx])) for idx, i in enumerate(I[0])]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "# Simulate an incoming transaction, retrieve similar historical cases, call Ollama, and persist result\n",
        "incoming = {\n",
        "    'tx_id': 'tx_live_0001',\n",
        "    'account_id': 'acct_9999',\n",
        "    'amount': 399.99,\n",
        "    'currency': 'USD',\n",
        "    'merchant': 'online_shop',\n",
        "    'description': 'large purchase at online_shop',\n",
        "}\n",
        "# compute embedding for incoming description\n",
        "q_vec = embedder.encode([incoming['description']], convert_to_numpy=True)[0].astype('float32')\n",
        "\n",
        "topk = retrieve_topk(q_vec, k=5)\n",
        "print('Retrieved top-k examples:', topk)\n",
        "\n",
        "# assemble a RAG prompt (simple template)\n",
        "retrieved_texts = []\n",
        "for txid, score in topk:\n",
        "    row = con.execute(\"SELECT tx_id, account_id, amount, merchant, description FROM transactions WHERE tx_id = ?\", (txid,)).fetchdf()\n",
        "    if len(row) > 0:\n",
        "        r = row.iloc[0]\n",
        "        retrieved_texts.append(f\"- {r.tx_id} | {r.merchant} | ${r.amount:.2f} | {r.description}\")\n",
        "\n",
        "prompt = f\"You are a fraud analyst assistant. Given the incoming transaction and similar historical transactions, output JSON with fields: {json.dumps({'risk_score':'float 0..1','explanation':'string','evidence_tx_ids':'list'})}.\\n\\n\"\n",
        "prompt += \"Incoming transaction:\\n\"\n",
        "prompt += f\"{incoming['tx_id']} | {incoming['merchant']} | ${incoming['amount']:.2f} | {incoming['description']}\\n\\n\"\n",
        "prompt += \"Retrieved similar historical transactions:\\n\"\n",
        "prompt += \"\\n\".join(retrieved_texts)\n",
        "prompt += \"\\n\\nReturn only valid JSON.\"\n",
        "\n",
        "print('\\nPrompt preview:\\n', prompt[:1000])\n",
        "\n",
        "# Call Ollama\n",
        "resp = call_ollama(OLLAMA_MODEL, prompt, temperature=0.0, timeout=60)\n",
        "print('Ollama call latency (ms):', resp['latency_ms'])\n",
        "\n",
        "# Try to parse JSON from response\n",
        "parsed = try_parse_json(resp['llm_response_raw'])\n",
        "risk_score = None\n",
        "evidence = None\n",
        "if parsed and isinstance(parsed, dict):\n",
        "    risk_score = parsed.get('risk_score')\n",
        "    evidence = parsed.get('evidence_tx_ids')\n",
        "else:\n",
        "    # if response not JSON, keep raw text\n",
        "    parsed = {'text': resp['llm_response_raw']}\n",
        "\n",
        "# Persist into llm_results\n",
        "row_id = resp['call_id']\n",
        "provenance = {\n",
        "    'emb_model': embed_model_name,\n",
        "    'index_backend': ann_backend,\n",
        "    'retrieved': [tx for tx, _ in topk]\n",
        "}\n",
        "con.execute(\n",
        "    \"\"\"\n",
        "    INSERT INTO llm_results (id, tx_id, llm_model, llm_provider, llm_prompt_hash, llm_prompt, llm_response, parsed_response, risk_score, evidence_tx_ids, call_latency_ms, provenance)\n",
        "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "    \"\"\",\n",
        "    (\n",
        "        row_id,\n",
        "        incoming['tx_id'],\n",
        "        resp['llm_model'],\n",
        "        resp['llm_provider'],\n",
        "        resp['prompt_hash'],\n",
        "        prompt if os.environ.get('STORE_PROMPTS','1')=='1' else None,\n",
        "        resp['llm_response_raw'],\n",
        "        json.dumps(parsed),\n",
        "        float(risk_score) if risk_score is not None else None,\n",
        "        json.dumps(evidence) if evidence is not None else json.dumps([t for t,_ in topk]),\n",
        "        int(resp['latency_ms']),\n",
        "        json.dumps(provenance)\n",
        "    )\n",
        ")\n",
        "print('Inserted llm_results id=', row_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "# Inspect persisted LLM results\n",
        "df_llm = con.execute('SELECT id, tx_id, llm_model, llm_provider, risk_score, created_at FROM llm_results ORDER BY created_at DESC LIMIT 10').df()\n",
        "df_llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next steps / notes:\n",
        "- Tune prompt templates and JSON schema enforcement for robust parsing.\n",
        "- Add retries, timeouts, and fallback for Ollama calls in production tasks.\n",
        "- Move index persistence to a vector DB (Milvus/Weaviate) when scaling beyond a single node.\n",
        "- Mask PII consistently and store PII mappings in a secure vault (HashiCorp Vault) if required.\n"
      ]
    }
  ]
}