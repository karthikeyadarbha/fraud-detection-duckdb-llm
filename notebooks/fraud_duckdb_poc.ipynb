{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEgLNW2XsSFG"
      },
      "source": [
        "# Fraud PoC â€” Robust LLM ingestion, parsing and repair\n",
        "\n",
        "This notebook contains integrated, ready-to-run cells to:\n",
        "- Backup the DuckDB file\n",
        "- Provide robust streaming assembly for Ollama responses\n",
        "- Parse numeric `risk_score` reliably\n",
        "- Insert per-transaction LLM results (one LLM call per tx)\n",
        "- Reprocess rows with missing/NaN `risk_score` (repair)\n",
        "\n",
        "Update DB_PATH below if your DB file is located elsewhere."
      ],
      "id": "dEgLNW2XsSFG"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a179611d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a179611d",
        "outputId": "f6e97805-18c4-40f2-fa52-3ae96cdf529b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DB_PATH = fraud_poc.duckdb\n",
            "OLLAMA_URL = http://localhost:11434/api/generate\n",
            "MODEL = gemma:2b\n"
          ]
        }
      ],
      "source": [
        "# CONFIG\n",
        "import os\n",
        "DB_PATH = os.environ.get('FRAUD_DB_PATH', 'fraud_poc.duckdb')   # change if needed\n",
        "OLLAMA_URL = os.environ.get('OLLAMA_URL', 'http://localhost:11434/api/generate')\n",
        "MODEL = os.environ.get('LLM_MODEL', 'gemma:2b') # Updated to gemma:2b\n",
        "print('DB_PATH =', DB_PATH)\n",
        "print('OLLAMA_URL =', OLLAMA_URL)\n",
        "print('MODEL =', MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhMAW5JXsrId",
        "outputId": "b6d43012-c885-4756-bd78-a6a66caeb2d5"
      },
      "id": "nhMAW5JXsrId",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 5136\n",
            "-rw-r--r-- 1 root root 5255168 Jan 11 06:17 fraud_poc.duckdb\n",
            "drwxr-xr-x 1 root root    4096 Dec  9 14:42 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "32f1d3e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32f1d3e3",
        "outputId": "c79be6c0-65d2-45ac-ccce-8d3379831e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backup created: fraud_poc.duckdb.bak\n"
          ]
        }
      ],
      "source": [
        "# Backup the DB file (run in notebook)\n",
        "import shutil\n",
        "if os.path.exists(DB_PATH):\n",
        "    bak = DB_PATH + '.bak'\n",
        "    shutil.copy2(DB_PATH, bak)\n",
        "    print(f'Backup created: {bak}')\n",
        "else:\n",
        "    raise FileNotFoundError(f'DB not found at {DB_PATH}; set DB_PATH correctly and run this cell again.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill ollama\n",
        "!curl -fsSL https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzncmkintbFG",
        "outputId": "a80ad6e4-f1d2-4877-97f7-d85666df76c6"
      },
      "id": "vzncmkintbFG",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tgz\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Set OLLAMA_HOST to allow access within the Colab environment\n",
        "os.environ['OLLAMA_HOST'] = '127.0.0.1:11434'\n",
        "\n",
        "# Start the Ollama server in the background using subprocess.Popen\n",
        "# 'nohup' and '&' ensure it runs continuously even if the cell finishes execution\n",
        "subprocess.Popen([\"nohup\", \"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Give the server a few seconds to start up\n",
        "time.sleep(5)\n",
        "print(\"Ollama server started.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlm_d7jktpiP",
        "outputId": "e590bfc0-deba-497a-d685-fc1dfc673756"
      },
      "id": "Jlm_d7jktpiP",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama server started.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eb02c380",
      "metadata": {
        "id": "eb02c380"
      },
      "outputs": [],
      "source": [
        "# Imports and robust parsing helpers\n",
        "import duckdb, json, re, math, datetime, uuid, requests\n",
        "import numpy as np\n",
        "\n",
        "def parse_risk_score(value):\n",
        "    \"\"\"Return float in 0..1 or math.nan if not parseable.\"\"\"\n",
        "    if value is None:\n",
        "        return math.nan\n",
        "    # numeric types\n",
        "    if isinstance(value, (int, float, np.integer, np.floating)):\n",
        "        v = float(value)\n",
        "        return math.nan if math.isnan(v) else v\n",
        "    s = str(value).strip()\n",
        "    # try JSON content\n",
        "    try:\n",
        "        obj = json.loads(s)\n",
        "        if isinstance(obj, dict):\n",
        "            # common keys\n",
        "            for key in (\"risk_score\",\"score\",\"risk\",\"riskScore\"):\n",
        "                if key in obj:\n",
        "                    return parse_risk_score(obj[key])\n",
        "        elif isinstance(obj, (int, float)):\n",
        "            return float(obj)\n",
        "    except Exception:\n",
        "        pass\n",
        "    low = s.lower()\n",
        "    if low in (\"\",\"null\",\"none\",\"n/a\",\"na\",\"nan\"):\n",
        "        return math.nan\n",
        "    # percent like 82%\n",
        "    m = re.search(r'(-?\\d+(?:[.,]\\d+)?)\\s*%', s)\n",
        "    if m:\n",
        "        try:\n",
        "            num = float(m.group(1).replace(',','.'))\n",
        "            return num/100.0\n",
        "        except:\n",
        "            return math.nan\n",
        "    # find first numeric token\n",
        "    m = re.search(r'(-?\\d+(?:[.,]\\d+)?)', s)\n",
        "    if m:\n",
        "        try:\n",
        "            num = float(m.group(1).replace(',','.'))\n",
        "        except:\n",
        "            return math.nan\n",
        "        if num < 0:\n",
        "            return math.nan\n",
        "        if num > 1 and num <= 100:\n",
        "            return num/100.0\n",
        "        return float(num)\n",
        "    return math.nan\n",
        "\n",
        "def extract_final_text_from_response(raw):\n",
        "    \"\"\"Attempt to get final textual output from a streaming llm_response field.\n",
        "    If raw contains newline-separated JSON lines, parse last JSON that has 'response' or numeric keys.\n",
        "    Otherwise return the last non-empty line or entire text as fallback.\n",
        "    \"\"\"\n",
        "    if raw is None:\n",
        "        return \"\"\n",
        "    text = str(raw)\n",
        "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
        "    # search from last line backwards\n",
        "    for ln in reversed(lines):\n",
        "        try:\n",
        "            obj = json.loads(ln)\n",
        "            if isinstance(obj, dict):\n",
        "                # direct numeric key\n",
        "                for key in (\"risk_score\",\"score\",\"risk\",\"riskScore\"):\n",
        "                    if key in obj:\n",
        "                        return obj[key]\n",
        "                if obj.get('response'):\n",
        "                    return obj['response']\n",
        "                if obj.get('thinking'):\n",
        "                    return obj['thinking']\n",
        "            elif isinstance(obj, (int,float)):\n",
        "                return obj\n",
        "        except Exception:\n",
        "            # not JSON; consider this line as candidate\n",
        "            if len(ln) > 0:\n",
        "                return ln\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6d2e52ef",
      "metadata": {
        "id": "6d2e52ef"
      },
      "outputs": [],
      "source": [
        "# Streaming wrapper to call Ollama and assemble final text (per prompt)\n",
        "def call_ollama_stream(prompt, model=MODEL, ollama_url=OLLAMA_URL, timeout=300):\n",
        "    payload = {\"model\": model, \"prompt\": prompt, \"temperature\": 0.0, \"max_tokens\": 512}\n",
        "    resp = requests.post(ollama_url, json=payload, stream=True, timeout=timeout)\n",
        "    resp.raise_for_status()\n",
        "    assembled = \"\"\n",
        "    raw_lines = []\n",
        "    for line in resp.iter_lines(decode_unicode=True):\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            chunk = json.loads(line)\n",
        "            raw_lines.append(chunk)\n",
        "        except Exception:\n",
        "            raw_lines.append({'text': line})\n",
        "            continue\n",
        "        if chunk.get('response'):\n",
        "            assembled += chunk['response']\n",
        "        elif chunk.get('thinking'):\n",
        "            assembled += chunk['thinking']\n",
        "        if chunk.get('done'):\n",
        "            break\n",
        "    return assembled, raw_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2b06a7ba",
      "metadata": {
        "id": "2b06a7ba"
      },
      "outputs": [],
      "source": [
        "# DB insert helper that stores parsed_response, llm_response, raw_lines, and flags needs_review\n",
        "def safe_insert_llm_result(con, row_id, tx_id, model, assembled, raw_lines, parsed_val, needs_review, now):\n",
        "    parsed_json = {\"parsed_risk\": None if math.isnan(parsed_val) else float(parsed_val)}\n",
        "    # Ensure needs_review column exists; add if missing\n",
        "    try:\n",
        "        con.execute(\"ALTER TABLE llm_results ADD COLUMN IF NOT EXISTS needs_review BOOLEAN DEFAULT FALSE\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    con.execute(\"\"\"\n",
        "        INSERT INTO llm_results (id, tx_id, llm_model, llm_response, parsed_response, risk_score, needs_review, created_at)\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "    \"\"\", (\n",
        "        row_id,\n",
        "        tx_id,\n",
        "        model,\n",
        "        assembled,\n",
        "        json.dumps(parsed_json),\n",
        "        (None if math.isnan(parsed_val) else float(parsed_val)),\n",
        "        needs_review,\n",
        "        now\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "08a110f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08a110f2",
        "outputId": "ed6b426c-2cb3-46fb-dadd-9933a72c7f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 unprocessed txs (limit 10).\n",
            "LLM call failed for tx tx_000031: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
            "LLM call failed for tx tx_000032: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
            "LLM call failed for tx tx_000033: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
            "LLM call failed for tx tx_000034: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
            "LLM call failed for tx tx_000035: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
            "LLM call failed for tx tx_000036: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
            "LLM call failed for tx tx_000037: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
            "LLM call failed for tx tx_000038: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
            "LLM call failed for tx tx_000039: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
            "LLM call failed for tx tx_000040: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-834549608.py:20: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  now = datetime.datetime.utcnow()\n"
          ]
        }
      ],
      "source": [
        "# Example: process unprocessed transactions (one LLM call per tx)\n",
        "con = duckdb.connect(DB_PATH)\n",
        "unprocessed_txs = con.execute(\"\"\"\n",
        "    SELECT t.tx_id, t.account_id, t.amount, t.currency, t.merchant, t.description\n",
        "    FROM transactions t\n",
        "    LEFT JOIN llm_results l ON t.tx_id = l.tx_id\n",
        "    WHERE l.id IS NULL\n",
        "    LIMIT 10\n",
        "\"\"\").fetchall()\n",
        "print(f\"Found {len(unprocessed_txs)} unprocessed txs (limit 10).\")\n",
        "for tx in unprocessed_txs:\n",
        "    tx_id, account_id, amount, currency, merchant, description = tx\n",
        "    prompt = f\"Transaction: account={account_id} amount={amount} {currency} merchant={merchant} description={description}\\n\\nReturn a numeric risk_score between 0 and 1 and a short explanation.\"\n",
        "    try:\n",
        "        assembled, raw_lines = call_ollama_stream(prompt)\n",
        "    except Exception as exc:\n",
        "        print(f\"LLM call failed for tx {tx_id}: {exc}\")\n",
        "        # insert placeholder row marked for review\n",
        "        row_id = str(uuid.uuid4())\n",
        "        now = datetime.datetime.utcnow()\n",
        "        safe_insert_llm_result(con, row_id, tx_id, MODEL, \"\", [{\"error\": str(exc)}], math.nan, True, now)\n",
        "        continue\n",
        "    parsed_val = parse_risk_score(assembled)\n",
        "    needs_review = False\n",
        "    if math.isnan(parsed_val) or parsed_val < 0 or parsed_val > 1:\n",
        "        needs_review = True\n",
        "    else:\n",
        "        parsed_val = max(0.0, min(1.0, float(parsed_val)))\n",
        "    row_id = str(uuid.uuid4())\n",
        "    now = datetime.datetime.utcnow()\n",
        "    safe_insert_llm_result(con, row_id, tx_id, MODEL, assembled, raw_lines, parsed_val, needs_review, now)\n",
        "    print(f\"Inserted: tx_id={tx_id} id={row_id} risk_score={parsed_val} needs_review={needs_review}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2427fe18",
        "outputId": "cc3e0461-8847-4a63-aa1a-f0931da98422"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "try:\n",
        "    # This will run the ollama list command and capture its output\n",
        "    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, check=True)\n",
        "    print(\"Ollama models listed successfully:\")\n",
        "    print(result.stdout)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error listing Ollama models: {e}\")\n",
        "    print(f\"Stdout: {e.stdout}\")\n",
        "    print(f\"Stderr: {e.stderr}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Ollama command not found. Please ensure Ollama is installed and in your PATH.\")\n"
      ],
      "id": "2427fe18",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama models listed successfully:\n",
            "NAME        ID              SIZE      MODIFIED       \n",
            "gemma:2b    b50d6c999e59    1.7 GB    11 seconds ago    \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0420cf3a",
        "outputId": "459633e8-ec10-4d52-d639-db3fe25de045"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "try:\n",
        "    # This will run the ollama pull command to install the specified model\n",
        "    result = subprocess.run(['ollama', 'pull', 'gemma:2b'], capture_output=True, text=True, check=True)\n",
        "    print(\"Ollama model 'gemma:2b' installed successfully:\")\n",
        "    print(result.stdout)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error installing Ollama model: {e}\")\n",
        "    print(f\"Stdout: {e.stdout}\")\n",
        "    print(f\"Stderr: {e.stderr}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Ollama command not found. Please ensure Ollama is installed and in your PATH.\")\n"
      ],
      "id": "0420cf3a",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama model 'gemma:2b' installed successfully:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57957467",
      "metadata": {
        "id": "57957467",
        "outputId": "33f404e9-69ad-4fa9-a9cf-7e0b9f5734bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows to reprocess: 3\n",
            "Reprocessed ea39f16f-25f5-414a-9f94-5e362757bee3: risk_score=1.0 needs_review=False\n",
            "Reprocessed 93b0a2e5-8a25-4a9c-b59b-0fd6cbe9d9ce: risk_score=0.7 needs_review=False\n",
            "Reprocessed 65f50de2-c08e-409a-bdff-a7e0f1b50c63: risk_score=0.0 needs_review=False\n"
          ]
        }
      ],
      "source": [
        "# Repair: reprocess rows with NULL/NaN risk_score if you have tx data available\n",
        "con = duckdb.connect(DB_PATH)\n",
        "to_reprocess = con.execute(\"\"\"\n",
        "SELECT l.id, l.tx_id, t.account_id, t.amount, t.currency, t.merchant, t.description\n",
        "FROM llm_results l\n",
        "LEFT JOIN transactions t ON l.tx_id = t.tx_id\n",
        "WHERE l.risk_score IS NULL OR (l.risk_score != l.risk_score)\n",
        "LIMIT 100\n",
        "\"\"\").fetchall()\n",
        "print(f\"Rows to reprocess: {len(to_reprocess)}\")\n",
        "for row in to_reprocess:\n",
        "    llm_id, tx_id, account_id, amount, currency, merchant, description = row\n",
        "    if tx_id is None:\n",
        "        print(f\"No tx data for llm result {llm_id}; skipping\")\n",
        "        continue\n",
        "    prompt = f\"Transaction: account={account_id} amount={amount} {currency} merchant={merchant} description={description}\\n\\nReturn a numeric risk_score between 0 and 1 and a short explanation.\"\n",
        "    try:\n",
        "        assembled, raw_lines = call_ollama_stream(prompt)\n",
        "    except Exception as exc:\n",
        "        print(f\"Reprocess failed for llm row {llm_id}: {exc}\")\n",
        "        continue\n",
        "    parsed_val = parse_risk_score(assembled)\n",
        "    needs_review = False\n",
        "    if math.isnan(parsed_val) or parsed_val < 0 or parsed_val > 1:\n",
        "        needs_review = True\n",
        "    else:\n",
        "        parsed_val = max(0.0, min(1.0, float(parsed_val)))\n",
        "    # update existing row\n",
        "    con.execute(\"\"\"\n",
        "        UPDATE llm_results\n",
        "        SET llm_response = ?, parsed_response = ?, risk_score = ?, needs_review = ?\n",
        "        WHERE id = ?\n",
        "    \"\"\", (assembled, json.dumps({\"parsed_risk\": None if math.isnan(parsed_val) else parsed_val}), (None if math.isnan(parsed_val) else parsed_val), needs_review, llm_id))\n",
        "    print(f\"Reprocessed {llm_id}: risk_score={parsed_val} needs_review={needs_review}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c36c11e",
      "metadata": {
        "id": "6c36c11e",
        "outputId": "68030602-fda7-4734-9938-59d364550fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total llm_results rows: 23\n",
            "Missing/NaN risk_score count: 0\n",
            "Needs review count: 2\n",
            "\n",
            "Sample needs_review rows:\n",
            "('f1f0792d-8653-4018-b210-29201423c886', 'tx_000016', -0.2, '{\"parsed_risk\": -0.2}', 'Okay, so I need to figure out a risk score between 0 and 1 for this transaction. Let me start by looking at the details given. The account is acct_0024, and the amount is -48.94 USD. The merchant is o')\n",
            "('77715f6b-ae5a-4783-8734-e2216bdc1e87', 'tx_000011', -0.15, '{\"parsed_risk\": -0.15}', 'Okay, so I need to figure out a risk score between 0 and 1 for this transaction. Let me start by looking at the details provided. The transaction is from account acct_0038 with an amount of $113.88 US')\n"
          ]
        }
      ],
      "source": [
        "# Diagnostics: show remaining missing / flagged rows\n",
        "con = duckdb.connect(DB_PATH)\n",
        "print('Total llm_results rows:', con.execute('SELECT COUNT(*) FROM llm_results').fetchone()[0])\n",
        "print('Missing/NaN risk_score count:', con.execute(\"SELECT COUNT(*) FROM llm_results WHERE risk_score IS NULL OR (risk_score != risk_score)\").fetchone()[0])\n",
        "print('Needs review count:', con.execute(\"SELECT COUNT(*) FROM llm_results WHERE needs_review = TRUE\").fetchone()[0])\n",
        "print('\\nSample needs_review rows:')\n",
        "rows = con.execute(\"SELECT id, tx_id, risk_score, parsed_response, SUBSTR(llm_response,1,200) FROM llm_results WHERE needs_review = TRUE ORDER BY created_at DESC LIMIT 10\").fetchall()\n",
        "for r in rows:\n",
        "    print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d20e1dae",
      "metadata": {
        "id": "d20e1dae"
      },
      "source": [
        "Notes:\n",
        "- The notebook assumes your `llm_results` table has columns: id, tx_id, llm_model, llm_response, parsed_response, risk_score, needs_review, created_at. If your schema differs, adjust the SQL and column names accordingly.\n",
        "- The `call_ollama_stream` function uses streaming to assemble text. It must be used once per transaction (or use batch prompts if you prefer).\n",
        "- After running this notebook, you should see NaNs reduced. Any rows where parsing still fails will be marked needs_review.\n",
        "If you want, I can also produce a PR that replaces the notebook in your repository with this version. Tell me whether you want a PR (and which target branch), or whether you'd prefer to paste these cells into your existing notebook yourself."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}