{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fraud-detection PoC: DuckDB + Embeddings + Retrieval + (Mock/Optional) LLM\n",
        "\n",
        "This notebook demonstrates a runnable end-to-end proof-of-concept using DuckDB for data management in a fraud-detection workflow. It covers:\n",
        "- Ingesting synthetic transactions into DuckDB\n",
        "- Computing text embeddings for transaction descriptions\n",
        "- Performing nearest-neighbour retrieval (semantic similarity)\n",
        "- Calling an LLM (mocked if no API key) to get an explanation and risk score\n",
        "- Storing LLM outputs and provenance back into DuckDB\n",
        "\n",
        "Notes:\n",
        "- This is a PoC for demonstration and data-management patterns; adapt storage, PII masking, and production vector stores (FAISS/Milvus) for production.\n",
        "- If you want real LLM calls, set the environment variable `OPENAI_API_KEY` before running and the notebook will call OpenAI's chat completion API; otherwise a deterministic mock LLM will be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Install dependencies (run once)\n",
        "# Uncomment the next line if you need to install packages in the environment running this notebook.\n",
        "# Note: model download (sentence-transformers) occurs in the embeddings cell and may take time.\n",
        "\n",
        "# !pip install -q duckdb sentence-transformers scikit-learn pandas numpy openai\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sentence_transformers import SentenceTransformer\n",
        "try:\n",
        "    import openai\n",
        "except Exception:\n",
        "    openai = None\n",
        "\n",
        "print('libraries loaded')\n",
        "\n",
        "# Config\n",
        "DB_FILE = ':memory:'  # for demo; replace with 'fraud_poc.duckdb' to persist to disk\n",
        "EMB_MODEL_NAME = 'all-MiniLM-L6-v2'  # small, fast SBERT model\n",
        "TOP_K = 5\n",
        "OPENAI_KEY = os.getenv('OPENAI_API_KEY')\n",
        "if OPENAI_KEY and openai:\n",
        "    openai.api_key = OPENAI_KEY\n",
        "    REAL_LLM_AVAILABLE = True\n",
        "else:\n",
        "    REAL_LLM_AVAILABLE = False\n",
        "\n",
        "print('REAL_LLM_AVAILABLE =', REAL_LLM_AVAILABLE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DuckDB connection and tables\n",
        "con = duckdb.connect(DB_FILE)\n",
        "\n",
        "con.execute('''\n",
        "CREATE TABLE IF NOT EXISTS transactions (\n",
        "  tx_id VARCHAR PRIMARY KEY,\n",
        "  account_id VARCHAR,\n",
        "  amount DOUBLE,\n",
        "  currency VARCHAR,\n",
        "  merchant VARCHAR,\n",
        "  merchant_mcc VARCHAR,\n",
        "  timestamp TIMESTAMP,\n",
        "  description VARCHAR,\n",
        "  device_info VARCHAR,\n",
        "  geo_country VARCHAR,\n",
        "  ingestion_job_id VARCHAR,\n",
        "  raw_source VARCHAR,\n",
        "  pii_masked BOOLEAN DEFAULT FALSE,\n",
        "  created_at TIMESTAMP DEFAULT current_timestamp\n",
        ");\n",
        "''')\n",
        "\n",
        "con.execute('''\n",
        "CREATE TABLE IF NOT EXISTS embeddings (\n",
        "  tx_id VARCHAR PRIMARY KEY,\n",
        "  emb_json VARCHAR,\n",
        "  emb_model VARCHAR,\n",
        "  emb_created_at TIMESTAMP DEFAULT current_timestamp,\n",
        "  emb_job_id VARCHAR\n",
        ");\n",
        "''')\n",
        "\n",
        "con.execute('''\n",
        "CREATE TABLE IF NOT EXISTS llm_results (\n",
        "  id VARCHAR PRIMARY KEY,\n",
        "  tx_id VARCHAR,\n",
        "  llm_model VARCHAR,\n",
        "  llm_provider VARCHAR,\n",
        "  llm_prompt_hash VARCHAR,\n",
        "  llm_prompt VARCHAR,\n",
        "  llm_response VARCHAR,\n",
        "  parsed_response JSON,\n",
        "  risk_score DOUBLE,\n",
        "  evidence_tx_ids VARCHAR,\n",
        "  call_latency_ms INTEGER,\n",
        "  usage JSON,\n",
        "  created_at TIMESTAMP DEFAULT current_timestamp\n",
        ");\n",
        "''')\n",
        "\n",
        "print('DuckDB tables created')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ingest some synthetic transactions (including labeled fraud examples)\n",
        "ingestion_job_id = hashlib.sha1(str(time.time()).encode()).hexdigest()[:10]\n",
        "sample = [\n",
        "    (\"tx1001\",\"acct1\", 120.0, \"USD\", \"MerchantA\",\"5812\",\"2025-12-01 10:00:00\",\"POS purchase at MerchantA\",\"chrome\",\"US\", ingestion_job_id, 'synthetic', False),\n",
        "    (\"tx1002\",\"acct2\", 9500.0, \"USD\", \"MerchantB\",\"6011\",\"2025-12-02 11:00:00\",\"Wire transfer to offshore account\",\"mobile\",\"CN\", ingestion_job_id, 'synthetic', False),\n",
        "    (\"tx1003\",\"acct3\", 45.0, \"USD\", \"MerchantC\",\"5999\",\"2025-12-03 09:00:00\",\"Online subscription monthly\",\"chrome\",\"US\", ingestion_job_id, 'synthetic', False),\n",
        "    (\"tx1004\",\"acct4\", 200.0, \"USD\", \"MerchantD\",\"5411\",\"2025-12-04 12:30:00\",\"Large POS purchase unusual location\",\"safari\",\"FR\", ingestion_job_id, 'synthetic', False),\n",
        "    (\"tx1005\",\"acct2\", 8800.0, \"USD\", \"MerchantE\",\"6011\",\"2025-12-05 15:10:00\",\"Transfer to newly added beneficiary\",\"mobile\",\"CN\", ingestion_job_id, 'synthetic', False)\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(sample, columns=[\"tx_id\",\"account_id\",\"amount\",\"currency\",\"merchant\",\"merchant_mcc\",\"timestamp\",\"description\",\"device_info\",\"geo_country\",\"ingestion_job_id\",\"raw_source\",\"pii_masked\"])\n",
        "con.execute(\"INSERT INTO transactions SELECT * FROM df\", {'df': df})\n",
        "print('Inserted synthetic transactions:')\n",
        "print(con.execute('SELECT tx_id, amount, description FROM transactions ORDER BY tx_id').fetchdf())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings for descriptions and store them in the embeddings table\n",
        "model = SentenceTransformer(EMB_MODEL_NAME)\n",
        "tx_df = con.execute(\"SELECT tx_id, description FROM transactions ORDER BY tx_id\").fetchdf()\n",
        "texts = tx_df['description'].astype(str).tolist()\n",
        "embs = model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "# Persist embeddings as JSON strings for PoC\n",
        "emb_job_id = hashlib.sha1(str(time.time()).encode()).hexdigest()[:10]\n",
        "rows = []\n",
        "for tx_id, emb in zip(tx_df['tx_id'].tolist(), embs):\n",
        "    rows.append((tx_id, json.dumps(emb.tolist()), EMB_MODEL_NAME, emb_job_id))\n",
        "\n",
        "emb_df = pd.DataFrame(rows, columns=['tx_id','emb_json','emb_model','emb_job_id'])\n",
        "con.execute('INSERT INTO embeddings SELECT * FROM emb_df', {'emb_df': emb_df})\n",
        "print('Stored', len(rows), 'embeddings in embeddings table')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a nearest-neighbour index (scikit-learn) in memory for retrieval\n",
        "emb_matrix = np.vstack([np.array(json.loads(x), dtype=np.float32) for x in con.execute('SELECT emb_json FROM embeddings ORDER BY tx_id').fetchdf()['emb_json']])\n",
        "tx_ids = con.execute('SELECT tx_id FROM embeddings ORDER BY tx_id').fetchdf()['tx_id'].tolist()\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=TOP_K, metric='cosine', algorithm='brute')\n",
        "nn.fit(emb_matrix)\n",
        "print('NearestNeighbors index built on', emb_matrix.shape[0], 'vectors')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define LLM caller (mock if no real LLM key)\n",
        "def call_llm(prompt: str, model_name: str = 'mock-llm', top_k_retrieved=None):\n",
        "    \"\"\"\n",
        "    Call an LLM provider if available, otherwise return a deterministic mock response.\n",
        "    Returns: dict with keys: text, parsed (dict or None), usage (dict or None), latency_ms (int)\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    if REAL_LLM_AVAILABLE and openai:\n",
        "        # Use OpenAI chat completion (gpt-3.5-turbo) as example\n",
        "        try:\n",
        "            resp = openai.ChatCompletion.create(\n",
        "                model='gpt-3.5-turbo',\n",
        "                messages=[{\"role\": \"system\", \"content\": \"You are a fraud-analyst assistant.\"}, {\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0,\n",
        "                max_tokens=400\n",
        "            )\n",
        "            text = resp['choices'][0]['message']['content']\n",
        "            usage = resp.get('usage', None)\n",
        "            latency_ms = int((time.time() - start) * 1000)\n",
        "            # Try to parse JSON from the response\n",
        "            parsed = None\n",
        "            try:\n",
        "                parsed = json.loads(text.strip())\n",
        "            except Exception:\n",
        "                # attempt to extract JSON substring\n",
        "                import re\n",
        "                m = re.search(r\"(\\{.*\\})\", text, re.DOTALL)\n",
        "                if m:\n",
        "                    try:\n",
        "                        parsed = json.loads(m.group(1))\n",
        "                    except Exception:\n",
        "                        parsed = None\n",
        "            return {\"text\": text, \"parsed\": parsed, \"usage\": usage, \"latency_ms\": latency_ms}\n",
        "        except Exception as e:\n",
        "            # fallback to mock if API call fails\n",
        "            print('LLM call failed, using mock. Error:', e)\n",
        "\n",
        "    # Mock deterministic response: derive risk_score from presence of labeled fraud in retrieved\n",
        "    retrieved = top_k_retrieved or []\n",
        "    # simple heuristic: if any retrieved label == 1 (we consider tx1002 and tx1005 as fraud-like in synthetic data)\n",
        "    fraud_like_ids = set(['tx1002','tx1005'])\n",
        "    evidence = [tid for tid in retrieved if tid in fraud_like_ids]\n",
        "    base_score = 0.2\n",
        "    if evidence:\n",
        "        base_score += 0.6\n",
        "    # increase score for large amount keyword\n",
        "    if 'transfer' in prompt.lower() or 'large' in prompt.lower():\n",
        "        base_score = min(0.95, base_score + 0.15)\n",
        "    parsed = {\"risk_score\": round(base_score, 2), \"explanation\": f\"Found similar fraud-like cases: {evidence}\", \"evidence\": evidence}\n",
        "    latency_ms = int((time.time() - start) * 1000)\n",
        "    return {\"text\": json.dumps(parsed), \"parsed\": parsed, \"usage\": None, \"latency_ms\": latency_ms}\n",
        "\n",
        "print('LLM caller defined (REAL_LLM_AVAILABLE=', REAL_LLM_AVAILABLE, ')')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate an incoming transaction that needs fraud evaluation\n",
        "incoming = {\n",
        "    'tx_id': 'tx_live_9001',\n",
        "    'account_id': 'acct1',\n",
        "    'amount': 8200.0,\n",
        "    'currency': 'USD',\n",
        "    'merchant': 'MerchantZ',\n",
        "    'merchant_mcc': '6011',\n",
        "    'timestamp': '2026-01-07 14:23:00',\n",
        "    'description': 'Immediate large transfer to new beneficiary account',\n",
        "    'device_info': 'mobile',\n",
        "    'geo_country': 'CN'\n",
        "}\n",
        "\n",
        "# Insert incoming transaction\n",
        "con.execute(\"INSERT INTO transactions (tx_id, account_id, amount, currency, merchant, merchant_mcc, timestamp, description, device_info, geo_country, ingestion_job_id, raw_source, pii_masked) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
        "            (incoming['tx_id'], incoming['account_id'], incoming['amount'], incoming['currency'], incoming['merchant'], incoming['merchant_mcc'], incoming['timestamp'], incoming['description'], incoming['device_info'], incoming['geo_country'], ingestion_job_id, 'realtime', False))\n",
        "print('Inserted incoming transaction', incoming['tx_id'])\n",
        "\n",
        "# Compute embedding for incoming description\n",
        "q_emb = model.encode([incoming['description']])[0].astype(np.float32)\n",
        "\n",
        "# Retrieve top-K similar historical transactions\n",
        "distances, indices = nn.kneighbors([q_emb], n_neighbors=TOP_K, return_distance=True)\n",
        "retrieved_indices = indices[0].tolist()\n",
        "retrieved_tx_ids = [tx_ids[i] for i in retrieved_indices]\n",
        "print('Retrieved tx_ids:', retrieved_tx_ids)\n",
        "\n",
        "# Get retrieved case details\n",
        "placeholders = ','.join([f\"'{tid}'\" for tid in retrieved_tx_ids]) if retrieved_tx_ids else \"''\"\n",
        "retrieved_df = con.execute(f\"SELECT tx_id, account_id, amount, merchant, description FROM transactions WHERE tx_id IN ({placeholders}) ORDER BY tx_id\").fetchdf()\n",
        "print('Retrieved details:\\n', retrieved_df)\n",
        "\n",
        "# Build prompt for RAG-style LLM call\n",
        "prompt = f\"\"\"\n",
        "You are a fraud analyst assistant. Given an incoming transaction and historical similar transactions, provide:\\n\n",
        "1) A risk score from 0.0 to 1.0 (1.0 = highest risk).\\n\n",
        "2) A short explanation (1-3 sentences) citing the retrieved transaction IDs and features that indicate fraud.\\n\n",
        "3) A JSON output with keys: risk_score, explanation, evidence (list of tx_ids).\\n\n",
        "Incoming transaction:\\n{json.dumps(incoming, indent=2)}\\n\n",
        "Retrieved historical cases:\\n{retrieved_df.to_json(orient='records')}\\n\n",
        "Return only valid JSON (object) as the top-level content.\n",
        "\"\"\"\n",
        "\n",
        "# Call LLM (real or mock)\n",
        "llm_start = time.time()\n",
        "llm_resp = call_llm(prompt, model_name='gpt-mock-or-api', top_k_retrieved=retrieved_tx_ids)\n",
        "llm_latency = llm_resp['latency_ms']\n",
        "llm_text = llm_resp['text']\n",
        "parsed = llm_resp['parsed']\n",
        "usage = llm_resp['usage']\n",
        "\n",
        "print('LLM response (parsed):', parsed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Persist LLM result with provenance into DuckDB\n",
        "import uuid\n",
        "res_id = str(uuid.uuid4())\n",
        "prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()\n",
        "risk_score = None\n",
        "if parsed and isinstance(parsed, dict):\n",
        "    risk_score = float(parsed.get('risk_score', None)) if parsed.get('risk_score', None) is not None else None\n",
        "\n",
        "con.execute(\n",
        "    \"INSERT INTO llm_results (id, tx_id, llm_model, llm_provider, llm_prompt_hash, llm_prompt, llm_response, parsed_response, risk_score, evidence_tx_ids, call_latency_ms, usage) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
        "    (\n",
        "        res_id,\n",
        "        incoming['tx_id'],\n",
        "        'gpt-mock-or-api',\n",
        "        'mock' if not REAL_LLM_AVAILABLE else 'openai',\n",
        "        prompt_hash,\n",
        "        prompt if REAL_LLM_AVAILABLE else None,  # store prompt only if allowed/real\n",
        "        llm_text,\n",
        "        json.dumps(parsed) if parsed is not None else None,\n",
        "        risk_score,\n",
        "        json.dumps(retrieved_tx_ids),\n",
        "        llm_latency,\n",
        "        json.dumps(usage) if usage is not None else None\n",
        "    )\n",
        ")\n",
        "\n",
        "print('Stored llm_result id=', res_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple enriched view (transactions + latest llm_result)\n",
        "con.execute('''\n",
        "CREATE OR REPLACE VIEW tx_enriched AS\n",
        "SELECT t.*, l.llm_model, l.llm_provider, l.llm_response, l.parsed_response, l.risk_score, l.evidence_tx_ids, l.created_at AS llm_ts\n",
        "FROM transactions t\n",
        "LEFT JOIN (\n",
        "  SELECT id, tx_id, llm_model, llm_provider, llm_response, parsed_response, risk_score, evidence_tx_ids, created_at\n",
        "  FROM llm_results\n",
        "  QUALIFY ROW_NUMBER() OVER (PARTITION BY tx_id ORDER BY created_at DESC) = 1\n",
        ") l USING (tx_id);\n",
        "''')\n",
        "\n",
        "print('Enriched view created. Sample high-risk items:')\n",
        "print(con.execute(\"SELECT tx_id, amount, description, risk_score FROM tx_enriched ORDER BY risk_score DESC NULLS LAST LIMIT 10\").fetchdf())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What you can do next\n",
        "\n",
        "- Replace the mock LLM with an on-prem or external LLM by setting OPENAI_API_KEY (or adding another provider) and adjusting the call_llm function.\n",
        "- For production scale, store embeddings in a dedicated vector store (FAISS/Milvus/Pinecone) and keep only references in DuckDB.\n",
        "- Add PII masking logic before any external API calls, and store prompt hashes if prompts must not be persisted.\n",
        "- Add data-quality checks, lineage writes, and a model registry for full governance.\n",
        "\n",
        "This notebook shows the core data-management pattern: treat LLM outputs as first-class data (store responses, parsed fields, provenance, and link them back to source records)."
      ]
    }
  ]
}