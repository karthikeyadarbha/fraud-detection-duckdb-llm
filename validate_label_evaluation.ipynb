{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa021b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline top100 frauds:      tp\n",
      "0  93.0\n",
      "Final top100 frauds:     tp\n",
      "0  1.0\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "q1 = \"\"\"\n",
    "SELECT SUM(CASE WHEN o.is_fraud THEN 1 ELSE 0 END) AS tp\n",
    "FROM (SELECT transaction_id FROM '/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet' ORDER BY baseline_score DESC LIMIT 100) r\n",
    "JOIN '/workspaces/fraud-detection-duckdb-llm/financial_fraud_detection_dataset.csv' o USING (transaction_id);\n",
    "\"\"\"\n",
    "q2 = \"\"\"\n",
    "SELECT SUM(CASE WHEN o.is_fraud THEN 1 ELSE 0 END) AS tp\n",
    "FROM (SELECT transaction_id FROM '/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet' ORDER BY final_score DESC LIMIT 100) r\n",
    "JOIN '/workspaces/fraud-detection-duckdb-llm/financial_fraud_detection_dataset.csv' o USING (transaction_id);\n",
    "\"\"\"\n",
    "print('Baseline top100 frauds:', duckdb.query(q1).fetchdf())\n",
    "print('Final top100 frauds:', duckdb.query(q2).fetchdf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f1a4d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id  baseline_score  anomaly_score  final_score  is_fraud\n",
      "0         T156071        0.000000            1.0          1.0     False\n",
      "1         T156146        0.000000            1.0          1.0     False\n",
      "2         T156697        0.000000            1.0          1.0     False\n",
      "3         T156784        0.000000            1.0          1.0     False\n",
      "4         T156918        0.000000            1.0          1.0     False\n",
      "5         T157400        0.000000            1.0          1.0     False\n",
      "6         T157629        0.000000            1.0          1.0     False\n",
      "7         T157752        0.000000            1.0          1.0     False\n",
      "8         T157860        0.000000            1.0          1.0     False\n",
      "9         T158165        0.000000            1.0          1.0     False\n",
      "10        T158180        0.000000            1.0          1.0     False\n",
      "11        T158233        0.000000            1.0          1.0     False\n",
      "12        T158244        0.000000            1.0          1.0     False\n",
      "13        T158417        0.000000            1.0          1.0     False\n",
      "14        T158712        0.000000            1.0          1.0     False\n",
      "15        T158876        0.000000            1.0          1.0     False\n",
      "16        T159033        0.000000            1.0          1.0     False\n",
      "17        T159093        0.000000            1.0          1.0     False\n",
      "18        T159143        0.000000            1.0          1.0     False\n",
      "19        T159204        0.000000            1.0          1.0     False\n",
      "20        T159236        0.000000            1.0          1.0     False\n",
      "21        T159356        0.000000            1.0          1.0     False\n",
      "22        T159391        0.000000            1.0          1.0     False\n",
      "23        T159402        0.000000            1.0          1.0     False\n",
      "24        T159532        0.000000            1.0          1.0     False\n",
      "25        T158490        0.000000            1.0          1.0     False\n",
      "26        T160147        0.046327            1.0          1.0     False\n",
      "27        T160252        0.000000            1.0          1.0     False\n",
      "28        T160282        0.000000            1.0          1.0     False\n",
      "29        T160349        0.000000            1.0          1.0     False\n",
      "30        T160373        0.000000            1.0          1.0     False\n",
      "31        T160499        0.000000            1.0          1.0     False\n",
      "32        T160641        0.000000            1.0          1.0     False\n",
      "33        T160646        0.000000            1.0          1.0     False\n",
      "34        T160767        0.000000            1.0          1.0     False\n",
      "35        T161118        0.000000            1.0          1.0     False\n",
      "36        T161405        0.000000            1.0          1.0     False\n",
      "37        T161540        0.000000            1.0          1.0     False\n",
      "38        T161777        0.000000            1.0          1.0     False\n",
      "39        T161822        0.000000            1.0          1.0     False\n",
      "40        T161927        0.000000            1.0          1.0     False\n",
      "41        T161930        0.000000            1.0          1.0     False\n",
      "42        T161986        0.000000            1.0          1.0     False\n",
      "43        T162035        0.043545            1.0          1.0     False\n",
      "44        T162170        0.000000            1.0          1.0     False\n",
      "45        T162233        0.000000            1.0          1.0     False\n",
      "46        T162379        0.000000            1.0          1.0     False\n",
      "47        T162405        0.000000            1.0          1.0     False\n",
      "48        T162425        0.000000            1.0          1.0     False\n",
      "49        T162426        0.000000            1.0          1.0     False\n"
     ]
    }
   ],
   "source": [
    "import duckdb \n",
    "q = \"\"\" SELECT r.transaction_id, r.baseline_score, r.anomaly_score, r.final_score, o.is_fraud FROM '/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet' r JOIN '/workspaces/fraud-detection-duckdb-llm/financial_fraud_detection_dataset.csv' o USING (transaction_id) ORDER BY r.final_score DESC LIMIT 50; \"\"\" \n",
    "print(duckdb.query(q).df()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5343f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_score {0.0: 0.0, 0.01: 0.0, 0.1: 0.0, 0.5: 0.0435471546350291, 0.9: 0.0451618229854689, 0.95: 0.0452653485952133, 0.99: 0.046326626111371, 1.0: 0.0655737704918032}\n",
      "anomaly_score {0.0: 0.0, 0.01: 0.002883526452322168, 0.1: 0.01916182418789604, 0.5: 0.9215508206289067, 0.9: 0.9607781988833756, 0.95: 0.9728427744390743, 0.99: 1.0, 1.0: 1.0}\n",
      "final_score {0.0: 0.0, 0.01: 0.009363344075304085, 0.1: 0.0435471546350291, 0.5: 0.9215508206289067, 0.9: 0.9607781988833756, 0.95: 0.9728427744390743, 0.99: 1.0, 1.0: 1.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet') \n",
    "for col in ['baseline_score','anomaly_score','final_score']: \n",
    "    print(col, df[col].quantile([0,0.01,0.1,0.5,0.9,0.95,0.99,1]).to_dict()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f0be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.csv rows: 5000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "p = '/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet'\n",
    "df = pd.read_parquet(p)\n",
    "##print(\"rows:\", len(df))\n",
    "##print(df.head(10).to_string(index=False))\n",
    "##print(\"\\nDecision counts:\\n\", df['decision'].value_counts())\n",
    "out = '/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.csv'\n",
    "df.to_csv(out, index=False)\n",
    "print(\"Wrote\", out, \"rows:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f889052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 5000000 positives: 179553\n",
      "Baseline AUC: 0.618114701100321\n",
      "Combined AUC: 0.519599362990919\n",
      "Baseline PR-AUC: 0.05420722358447542\n",
      "Combined PR-AUC: 0.03767990075434456\n",
      "Precision@50: baseline=0.520, final=0.100\n",
      "Precision@100: baseline=0.520, final=0.050\n",
      "Precision@500: baseline=0.554, final=0.032\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "##res = pd.read_parquet('artifacts/results_stream.parquet')\n",
    "orig = pd.read_csv('/workspaces/fraud-detection-duckdb-llm/financial_fraud_detection_dataset.csv', usecols=['transaction_id','is_fraud'])\n",
    "m = orig.merge(df, on='transaction_id', how='left')\n",
    "y = m['is_fraud'].astype(str).str.upper().map({'TRUE':1,'FALSE':0}).fillna(0).astype(int)\n",
    "print(\"rows:\", len(m), \"positives:\", y.sum())\n",
    "print(\"Baseline AUC:\", roc_auc_score(y, m['baseline_score']))\n",
    "print(\"Combined AUC:\", roc_auc_score(y, m['final_score']))\n",
    "print(\"Baseline PR-AUC:\", average_precision_score(y, m['baseline_score']))\n",
    "print(\"Combined PR-AUC:\", average_precision_score(y, m['final_score']))\n",
    "for k in (50,100,500):\n",
    "    topb = m.sort_values('baseline_score', ascending=False).head(k)['is_fraud'].astype(str).str.upper().map({'TRUE':1,'FALSE':0}).sum()\n",
    "    topf = m.sort_values('final_score', ascending=False).head(k)['is_fraud'].astype(str).str.upper().map({'TRUE':1,'FALSE':0}).sum()\n",
    "    print(f\"Precision@{k}: baseline={(topb/k):.3f}, final={(topf/k):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4991acf",
   "metadata": {},
   "outputs": [
    {
     "ename": "BinderException",
     "evalue": "Binder Error: No function matches the given name and argument types '~~*(BOOLEAN, STRING_LITERAL)'. You might need to add explicit type casts.\n\tCandidate functions:\n\t~~*(VARCHAR, VARCHAR) -> BOOLEAN\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBinderException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mduckdb\u001b[39;00m\n\u001b[32m      2\u001b[39m q = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mSELECT\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m  (SELECT SUM(CASE WHEN o.is_fraud ILIKE \u001b[39m\u001b[33m'\u001b[39m\u001b[33mTRUE\u001b[39m\u001b[33m'\u001b[39m\u001b[33m THEN 1 ELSE 0 END) FROM (SELECT transaction_id FROM \u001b[39m\u001b[33m'\u001b[39m\u001b[33m/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m ORDER BY baseline_score DESC LIMIT 100) r JOIN \u001b[39m\u001b[33m'\u001b[39m\u001b[33m/workspaces/fraud-detection-duckdb-llm/financial_fraud_detection_dataset.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m o USING(transaction_id)) AS baseline_top100_tp,\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m  (SELECT SUM(CASE WHEN o.is_fraud ILIKE \u001b[39m\u001b[33m'\u001b[39m\u001b[33mTRUE\u001b[39m\u001b[33m'\u001b[39m\u001b[33m THEN 1 ELSE 0 END) FROM (SELECT transaction_id FROM \u001b[39m\u001b[33m'\u001b[39m\u001b[33m/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m ORDER BY final_score DESC LIMIT 100) r JOIN \u001b[39m\u001b[33m'\u001b[39m\u001b[33m/workspaces/fraud-detection-duckdb-llm/financial_fraud_detection_dataset.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m o USING(transaction_id)) AS final_top100_tp\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mduckdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m.fetchdf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/fraud-poc/lib/python3.12/site-packages/duckdb/__init__.py:465\u001b[39m, in \u001b[36mquery\u001b[39m\u001b[34m(query, **kwargs)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    464\u001b[39m     conn = duckdb.connect(\u001b[33m\"\u001b[39m\u001b[33m:default:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mBinderException\u001b[39m: Binder Error: No function matches the given name and argument types '~~*(BOOLEAN, STRING_LITERAL)'. You might need to add explicit type casts.\n\tCandidate functions:\n\t~~*(VARCHAR, VARCHAR) -> BOOLEAN\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "q = \"\"\"\n",
    "SELECT\n",
    "  (SELECT SUM(CASE WHEN o.is_fraud ILIKE 'TRUE' THEN 1 ELSE 0 END) FROM (SELECT transaction_id FROM '/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet' ORDER BY baseline_score DESC LIMIT 100) r JOIN '/workspaces/fraud-detection-duckdb-llm/financial_fraud_detection_dataset.csv' o USING(transaction_id)) AS baseline_top100_tp,\n",
    "  (SELECT SUM(CASE WHEN o.is_fraud ILIKE 'TRUE' THEN 1 ELSE 0 END) FROM (SELECT transaction_id FROM '/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet' ORDER BY final_score DESC LIMIT 100) r JOIN '/workspaces/fraud-detection-duckdb-llm/financial_fraud_detection_dataset.csv' o USING(transaction_id)) AS final_top100_tp\n",
    "\"\"\"\n",
    "print(duckdb.query(q).fetchdf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214d57e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   baseline_top100_tp  final_top100_tp\n",
      "0                93.0              4.0\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT\n",
    "  (SELECT SUM(CASE WHEN o.is_fraud = TRUE THEN 1 ELSE 0 END)\n",
    "     FROM (SELECT transaction_id FROM 'artifacts/results_stream.parquet' ORDER BY baseline_score DESC LIMIT 100) r\n",
    "     JOIN 'financial_fraud_detection_dataset.csv' o USING (transaction_id)\n",
    "  ) AS baseline_top100_tp,\n",
    "  (SELECT SUM(CASE WHEN o.is_fraud = TRUE THEN 1 ELSE 0 END)\n",
    "     FROM (SELECT transaction_id FROM 'artifacts/results_stream.parquet' ORDER BY final_score DESC LIMIT 100) r\n",
    "     JOIN 'financial_fraud_detection_dataset.csv' o USING (transaction_id)\n",
    "  ) AS final_top100_tp;\n",
    "\"\"\"\n",
    "\n",
    "print(duckdb.query(q).fetchdf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7b9aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /workspaces/fraud-detection-duckdb-llm/artifacts/results_stream_baseline_only.{parquet,csv}, rows: 5000000\n",
      "allow    5000000\n",
      "Name: decision, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "p='/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet'\n",
    "df = pd.read_parquet(p) \n",
    "df['final_score'] = df['baseline_score'] \n",
    "df['decision'] = df['final_score'].apply(lambda s: 'block' if s>=0.9 else ('review' if s>=0.6 else 'allow')) \n",
    "df.to_parquet('/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream_baseline_only.parquet', index=False) \n",
    "df.to_csv('/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream_baseline_only.csv', index=False) \n",
    "print(\"Wrote /workspaces/fraud-detection-duckdb-llm/artifacts/results_stream_baseline_only.{parquet,csv}, rows:\", len(df)) \n",
    "print(df['decision'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467e6693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   baseline_top100_tp  final_top100_tp\n",
      "0                   3                4\n"
     ]
    }
   ],
   "source": [
    "import duckdb \n",
    "q = \"\"\" SELECT (SELECT COUNT() FROM (SELECT transaction_id FROM '/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet' ORDER BY baseline_score DESC LIMIT 100) r JOIN '/workspaces/fraud-detection-duckdb-llm/financial_fraud_detection_dataset.csv' o USING (transaction_id) WHERE o.is_fraud = TRUE) AS baseline_top100_tp, (SELECT COUNT() FROM (SELECT transaction_id FROM '/workspaces/fraud-detection-duckdb-llm/artifacts/results_stream.parquet' ORDER BY final_score DESC LIMIT 100) r JOIN '/workspaces/fraud-detection-duckdb-llm/financial_fraud_detection_dataset.csv' o USING (transaction_id) WHERE o.is_fraud = TRUE) AS final_top100_tp; \"\"\" \n",
    "print(duckdb.query(q).fetchdf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dbbbb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      k  baseline_tp  final_tp  overlap_topk\n",
      "0    50            3         2             0\n",
      "1   100           93         4             0\n",
      "2   500          483         0             0\n",
      "3  1000          725        31             3\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# compute TP counts and precision@k for several ks and overlap between top lists\n",
    "ks = [50,100,500,1000]\n",
    "queries = []\n",
    "for k in ks:\n",
    "    queries.append(f\"\"\"\n",
    "    SELECT\n",
    "      {k} AS k,\n",
    "      (SELECT COUNT(*) FROM (SELECT transaction_id FROM 'artifacts/results_stream.parquet' ORDER BY baseline_score DESC LIMIT {k}) r\n",
    "         JOIN 'financial_fraud_detection_dataset.csv' o USING (transaction_id) WHERE o.is_fraud = TRUE) AS baseline_tp,\n",
    "      (SELECT COUNT(*) FROM (SELECT transaction_id FROM 'artifacts/results_stream.parquet' ORDER BY final_score DESC LIMIT {k}) r\n",
    "         JOIN 'financial_fraud_detection_dataset.csv' o USING (transaction_id) WHERE o.is_fraud = TRUE) AS final_tp,\n",
    "      (SELECT COUNT(*) FROM\n",
    "         (SELECT transaction_id FROM 'artifacts/results_stream.parquet' ORDER BY baseline_score DESC LIMIT {k}) b\n",
    "         JOIN (SELECT transaction_id FROM 'artifacts/results_stream.parquet' ORDER BY final_score DESC LIMIT {k}) f USING (transaction_id)\n",
    "      ) AS overlap_topk\n",
    "    \"\"\")\n",
    "q = \" UNION ALL \".join(queries) + \";\"\n",
    "print(duckdb.query(q).fetchdf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04210d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sample recalibrated output found: artifacts/results_stream_sample_recalibrated.parquet\n",
      "\n",
      "No sample manifest found: artifacts/manifest_ae_anomaly_e40c79565705.sample.updated.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, json, os\n",
    "p = 'artifacts/results_stream_sample_recalibrated.parquet'\n",
    "if os.path.exists(p):\n",
    "    df = pd.read_parquet(p)\n",
    "    print(\"rows:\", len(df))\n",
    "    print(\"anomaly quantiles:\", df['anomaly_score'].quantile([0.01,0.1,0.5,0.9,0.99]).to_dict())\n",
    "    print(\"baseline quantiles:\", df['baseline_score'].quantile([0.01,0.1,0.5,0.9,0.99]).to_dict())\n",
    "    print(\"final quantiles:\", df['final_score'].quantile([0.01,0.1,0.5,0.9,0.99]).to_dict())\n",
    "else:\n",
    "    print(\"No sample recalibrated output found:\", p)\n",
    "\n",
    "mfile = 'artifacts/manifest_ae_anomaly_e40c79565705.sample.updated.json'\n",
    "if os.path.exists(mfile):\n",
    "    print(\"\\nmanifest:\", json.load(open(mfile)))\n",
    "else:\n",
    "    print(\"\\nNo sample manifest found:\", mfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "440776ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact exists: True\n",
      "Loaded artifact type: <class 'dict'>\n",
      "Top-level keys: ['scaler', 'features', 'manifest', 'encoder', 'model_file']\n",
      "\n",
      "--- model_file ---\n",
      "artifacts/ae_anomaly_e40c79565705.keras exists: True\n",
      "\n",
      "--- scaler ---\n",
      "<class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "\n",
      "--- features ---\n",
      "<class 'list'>\n",
      "\n",
      "--- manifest ---\n",
      "<class 'dict'>\n",
      "\n",
      "manifest snippet:\n",
      "{'p1_mse': 0.04913485795259476, 'p99_mse': 1.0583398342132568}\n"
     ]
    }
   ],
   "source": [
    "import os, joblib, json, pprint, traceback\n",
    "p = 'artifacts/ae_anomaly_e40c79565705.pkl'\n",
    "print(\"artifact exists:\", os.path.exists(p))\n",
    "try:\n",
    "    a = joblib.load(p)\n",
    "    print(\"Loaded artifact type:\", type(a))\n",
    "    print(\"Top-level keys:\", list(a.keys()))\n",
    "    # show relevant fields if present\n",
    "    for k in ('model_file','model_dir','scaler','features','manifest','iso'):\n",
    "        if k in a:\n",
    "            print(f\"\\n--- {k} ---\")\n",
    "            v = a[k]\n",
    "            if isinstance(v, (str,)):\n",
    "                print(v, \"exists:\", os.path.exists(v))\n",
    "            else:\n",
    "                try:\n",
    "                    pprint.pprint(type(v))\n",
    "                except Exception:\n",
    "                    print(\"type:\", type(v))\n",
    "    # print manifest summary if present\n",
    "    if 'manifest' in a:\n",
    "        print(\"\\nmanifest snippet:\")\n",
    "        pprint.pprint({kk:a['manifest'].get(kk) for kk in ['p1_mse','p99_mse'] if kk in a['manifest']})\n",
    "except Exception as e:\n",
    "    print(\"Failed to load/inspect anomaly artifact:\", e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e22867e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load TF model from: artifacts/ae_anomaly_e40c79565705.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 11:50:05.605766: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-18 11:50:10.292743: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-18 11:50:12.229292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768737015.790291   29853 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768737016.665672   29853 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768737023.887482   29853 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768737023.887514   29853 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768737023.887522   29853 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768737023.887527   29853 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-18 11:50:24.272679: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-18 11:50:36.333822: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TF model; summary (first 3 lines):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tabular_autoencoder\"\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ input_layer (InputLayer)        │ (None, 279)            │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (Dense)                   │ (None, 558)            │       156,240 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense_1 (Dense)                 │ (None, 279)            │       155,961 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ bottleneck (Dense)              │ (None, 8)              │         2,240 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense_2 (Dense)                 │ (None, 279)            │         2,511 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense_3 (Dense)                 │ (None, 558)            │       156,240 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense_4 (Dense)                 │ (None, 279)            │       155,961 │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      " Total params: 629,153 (2.40 MB)\n",
      " Trainable params: 629,153 (2.40 MB)\n",
      " Non-trainable params: 0 (0.00 B)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, joblib, traceback\n",
    "try:\n",
    "    an = joblib.load('artifacts/ae_anomaly_e40c79565705.pkl')\n",
    "    model_path = an.get('model_file') or an.get('model_dir')\n",
    "    print(\"Attempting to load TF model from:\", model_path)\n",
    "    import tensorflow as tf\n",
    "    m = tf.keras.models.load_model(model_path, compile=False)\n",
    "    print(\"Loaded TF model; summary (first 3 lines):\")\n",
    "    s = []\n",
    "    m.summary(print_fn=lambda x: s.append(x))\n",
    "    print(\"\\n\".join(s[:3]))\n",
    "except Exception as e:\n",
    "    print(\"TF model load/predict failed:\", e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dd473c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IOException",
     "evalue": "IO Error: No files found that match the pattern \"artifacts/results_stream_recalibrated.parquet\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIOException\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mduckdb\u001b[39;00m\n\u001b[32m      2\u001b[39m q = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mSELECT\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m  (SELECT COUNT(*) FROM (SELECT transaction_id FROM \u001b[39m\u001b[33m'\u001b[39m\u001b[33martifacts/results_stream_recalibrated.parquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m ORDER BY baseline_score DESC LIMIT 100) r\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[33m   JOIN \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfinancial_fraud_detection_dataset.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m o USING (transaction_id) WHERE o.is_fraud = TRUE) AS final_top100_tp;\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mduckdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m.fetchdf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/fraud-poc/lib/python3.12/site-packages/duckdb/__init__.py:465\u001b[39m, in \u001b[36mquery\u001b[39m\u001b[34m(query, **kwargs)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    464\u001b[39m     conn = duckdb.connect(\u001b[33m\"\u001b[39m\u001b[33m:default:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIOException\u001b[39m: IO Error: No files found that match the pattern \"artifacts/results_stream_recalibrated.parquet\""
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "q = \"\"\"\n",
    "SELECT\n",
    "  (SELECT COUNT(*) FROM (SELECT transaction_id FROM 'artifacts/results_stream_recalibrated.parquet' ORDER BY baseline_score DESC LIMIT 100) r\n",
    "   JOIN 'financial_fraud_detection_dataset.csv' o USING (transaction_id) WHERE o.is_fraud = TRUE) AS baseline_top100_tp,\n",
    "  (SELECT COUNT(*) FROM (SELECT transaction_id FROM 'artifacts/results_stream_recalibrated.parquet' ORDER BY final_score DESC LIMIT 100) r\n",
    "   JOIN 'financial_fraud_detection_dataset.csv' o USING (transaction_id) WHERE o.is_fraud = TRUE) AS final_top100_tp;\n",
    "\"\"\"\n",
    "print(duckdb.query(q).fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfe0ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4510d38b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'artifacts/ae_recon_errors_sample.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m errors = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43martifacts/ae_recon_errors_sample.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mrecon_error\u001b[39m\u001b[33m'\u001b[39m] \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(\u001b[33m'\u001b[39m\u001b[33martifacts/ae_recon_errors_sample.parquet\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTest\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/fraud-poc/lib/python3.12/site-packages/pandas/io/parquet.py:503\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[33;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    458\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    499\u001b[39m \u001b[33;03mDataFrame\u001b[39;00m\n\u001b[32m    500\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    501\u001b[39m impl = get_engine(engine)\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/fraud-poc/lib/python3.12/site-packages/pandas/io/parquet.py:244\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    242\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m path_or_handle, handles, kwargs[\u001b[33m\"\u001b[39m\u001b[33mfilesystem\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfilesystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    251\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    252\u001b[39m         path_or_handle, columns=columns, **kwargs\n\u001b[32m    253\u001b[39m     ).to_pandas(**to_pandas_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/fraud-poc/lib/python3.12/site-packages/pandas/io/parquet.py:102\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m     92\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     94\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m     95\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    106\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/fraud-poc/lib/python3.12/site-packages/pandas/io/common.py:865\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    856\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    857\u001b[39m             handle,\n\u001b[32m    858\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    861\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    862\u001b[39m         )\n\u001b[32m    863\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    864\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m     handles.append(handle)\n\u001b[32m    868\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'artifacts/ae_recon_errors_sample.parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd, json, numpy as np\n",
    "errors = pd.read_parquet('artifacts/ae_recon_errors_sample.parquet')['recon_error'] \n",
    "if os.path.exists('artifacts/ae_recon_errors_sample.parquet'):\n",
    "    print('Test')\n",
    "else: \n",
    "    None \n",
    "print(\"errors available:\", errors is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be3ccda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote artifacts/topk_evidence.parquet with 1000 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, json\n",
    "df = pd.read_parquet('artifacts/results_stream.parquet').head(1000)  # or sample\n",
    "out = []\n",
    "for _, r in df.iterrows():\n",
    "    tx = str(r['transaction_id'])\n",
    "    # Build 3 short evidence items (replace with real retrieval in prod)\n",
    "    topk = [\n",
    "        {\"id\": \"E1\", \"text\": f\"amount={r.get('amount','?')}; baseline={r.get('baseline_score'):.4f}\" if 'amount' in r else f\"baseline={r.get('baseline_score'):.4f}\"},\n",
    "        {\"id\": \"E2\", \"text\": f\"merchant_risk={r.get('merchant_risk','?')}\"},\n",
    "        {\"id\": \"E3\", \"text\": \"recent device mismatch\" }\n",
    "    ]\n",
    "    out.append({\"transaction_id\": tx, \"top_k_evidence\": topk})\n",
    "# write as parquet with JSON strings (the Level-3 runner supports object lists)\n",
    "pd.DataFrame(out).to_parquet('artifacts/topk_evidence.parquet', index=False)\n",
    "print(\"wrote artifacts/topk_evidence.parquet with\", len(out), \"rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
