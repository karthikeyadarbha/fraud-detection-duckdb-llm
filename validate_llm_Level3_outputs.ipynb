{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6f8c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 1000\n",
      "valid adjustments: 0\n",
      "needs_review: 1000\n",
      "llm_adj clamped stats:\n",
      " count    1000.0\n",
      "mean        0.0\n",
      "std         0.0\n",
      "min         0.0\n",
      "25%         0.0\n",
      "50%         0.0\n",
      "75%         0.0\n",
      "max         0.0\n",
      "example evidence ids for first rows:\n",
      " transaction_id llm_evidence_ids\n",
      "       T100000               []\n",
      "       T100001               []\n",
      "       T100002               []\n",
      "       T100003               []\n",
      "       T100004               []\n",
      "       T100005               []\n",
      "       T100006               []\n",
      "       T100007               []\n",
      "       T100008               []\n",
      "       T100009               []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "p='artifacts/level3_llm/llm_parsed.parquet'\n",
    "df=pd.read_parquet(p)\n",
    "print(\"rows:\", len(df))\n",
    "print(\"valid adjustments:\", int(df['llm_adjustment_valid'].sum()))\n",
    "print(\"needs_review:\", int(df['needs_review'].sum()))\n",
    "print(\"llm_adj clamped stats:\\n\", df['llm_adjustment_clamped'].describe().to_string())\n",
    "print(\"example evidence ids for first rows:\\n\", df[['transaction_id','llm_evidence_ids']].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ac32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a00f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT (truncated):\n",
      "\n",
      "You are a strict fraud-triage assistant. Answer ONLY with a single valid JSON object that matches the schema exactly.\n",
      "\n",
      "Schema:\n",
      "{\n",
      "  \"transaction_id\": \"<string>\",\n",
      "  \"llm_adjustment\": <number>,       # additive delta to baseline score (range -0.10 .. +0.10). Will be clamped by caller to +/-0.0500\n",
      "  \"evidence_ids\": [\"E1\",\"E2\"],     # IDs referencing provided evidence items (must be subset of provided top_k_evidence ids)\n",
      "  \"explanation\": \"<string>\",        # concise factual explanation based ONLY on provided evidence (<=250 chars)\n",
      "  \"confidence\": <number>           # optional 0..1 numeric confidence\n",
      "}\n",
      "\n",
      "Rules:\n",
      "- DO NOT output any text outside the JSON object (no markdown, no comments).\n",
      "- Only reference evidence by id. evidence_ids MUST be a subset of the provided top_k_evidence ids.\n",
      "- llm_adjustment must be a numeric additive delta; we will clamp it to +/- the configured max_delta.\n",
      "- explanation must be concise and based solely on the evidence.\n",
      "- Temperature must be 0 (deterministic).\n",
      "- If \n",
      "\n",
      "RAW_OUTPUT:\n",
      "__OLLAMA_ERROR__ RuntimeError('ollama generate failed: rc=1, stderr=Error: unknown command \"generate\" for \"ollama\"\\n')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "lines = open('artifacts/level3_llm/llm_raw.jsonl','r',encoding='utf-8').read().splitlines()\n",
    "if not lines:\n",
    "    print(\"no raw lines\")\n",
    "else:\n",
    "    j = json.loads(lines[0])\n",
    "    print(\"PROMPT (truncated):\")\n",
    "    print(j.get('prompt','')[:1000])\n",
    "    print(\"\\nRAW_OUTPUT:\")\n",
    "    print(j.get('raw_output','')[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87cfc7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 5\n",
      "valid adjustments: 0\n",
      "needs_review: 5\n",
      "llm_adj clamped stats:\n",
      " count    5.0\n",
      "mean     0.0\n",
      "std      0.0\n",
      "min      0.0\n",
      "25%      0.0\n",
      "50%      0.0\n",
      "75%      0.0\n",
      "max      0.0\n",
      "transaction_id llm_evidence_ids  llm_adjustment_clamped  needs_review\n",
      "       T100000               []                     0.0          True\n",
      "       T100001               []                     0.0          True\n",
      "       T100002               []                     0.0          True\n",
      "       T100003               []                     0.0          True\n",
      "       T100004               []                     0.0          True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "p='artifacts/level3_llm_test/llm_parsed.parquet'\n",
    "df=pd.read_parquet(p)\n",
    "print(\"rows:\", len(df))\n",
    "print(\"valid adjustments:\", int(df['llm_adjustment_valid'].sum()))\n",
    "print(\"needs_review:\", int(df['needs_review'].sum()))\n",
    "print(\"llm_adj clamped stats:\\n\", df['llm_adjustment_clamped'].describe().to_string())\n",
    "print(df[['transaction_id','llm_evidence_ids','llm_adjustment_clamped','needs_review']].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6960d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"transaction_id\": \"T100000\",\n",
      "  \"timestamp_utc\": \"2026-01-24T15:03:13.196882Z\",\n",
      "  \"prompt\": \"\\nYou are a strict fraud-triage assistant. Answer ONLY with a single valid JSON object that matches the schema exactly.\\n\\nSchema:\\n{\\n  \\\"transaction_id\\\": \\\"<string>\\\",\\n  \\\"llm_adjustment\\\": <number>,       # additive delta to baseline score (range -0.10 .. +0.10). Will be clamped by caller to +/-0.0500\\n  \\\"evidence_ids\\\": [\\\"E1\\\",\\\"E2\\\"],     # IDs referencing provided evidence items (must be subset of provided top_k_evidence ids)\\n  \\\"explanation\\\": \\\"<string>\\\",        # concise factual explanation based ONLY on provided evidence (<=250 chars)\\n  \\\"confidence\\\": <number>           # optional 0..1 numeric confidence\\n}\\n\\nRules:\\n- DO NOT output any text outside the JSON object (no markdown, no comments).\\n- Only reference evidence by id. evidence_ids MUST be a subset of the provided top_k_evidence ids.\\n- llm_adjustment must be a numeric additive delta; we will clamp it to +/- the configured max_delta.\\n- explanation must be concise and based solely on the evidence.\\n- Temperature must be 0 (deterministic).\\n- If you cannot produce a valid JSON that follows these rules, output the most faithful JSON indicating the reason, but keep the JSON schema shape.\\n\\nInput:\\n{\\n  \\\"transaction_id\\\": \\\"T100000\\\",\\n  \\\"baseline_score\\\": 0.0,\\n  \\\"features\\\": {\\n    \\\"anomaly_score\\\": 0.9201052578982496,\\n    \\\"llm_adjustment\\\": 0.0,\\n    \\\"final_score\\\": 0.9201052578982496,\\n    \\\"decision\\\": \\\"block\\\"\\n  },\\n  \\\"top_k_evidence\\\": [\\n    {\\n      \\\"id\\\": \\\"E1\\\",\\n      \\\"text\\\": \\\"baseline=0.0000\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"E2\\\",\\n      \\\"text\\\": \\\"merchant_risk=?\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"E3\\\",\\n      \\\"text\\\": \\\"recent device mismatch\\\"\\n    }\\n  ],\\n  \\\"max_delta\\\": 0.05\\n}\\n\\nRespond with the JSON object only.\\n\",\n",
      "  \"raw_output\": \"__NO_RUNNER__\",\n",
      "  \"model\": \"llama2\",\n",
      "  \"elapsed_seconds\": 7.152557373046875e-07\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "line = open('artifacts/level3_llm_test/llm_raw.jsonl','r',encoding='utf-8').readline()\n",
    "print(json.dumps(json.loads(line), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba62c341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WROTE: artifacts/results_stream_with_llm.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "res = pd.read_parquet('artifacts/results_stream.parquet')\n",
    "llm = pd.read_parquet('artifacts/level3_llm_test/llm_parsed.parquet')\n",
    "\n",
    "res['transaction_id'] = res['transaction_id'].astype(str)\n",
    "llm['transaction_id'] = llm['transaction_id'].astype(str)\n",
    "\n",
    "llm_sel = llm[['transaction_id','llm_adjustment_clamped','llm_adjustment_raw','llm_adjustment_valid','llm_evidence_ids','needs_review']].copy()\n",
    "merged = res.merge(llm_sel, on='transaction_id', how='left')\n",
    "\n",
    "merged['llm_adjustment_clamped'] = merged['llm_adjustment_clamped'].fillna(0.0)\n",
    "merged['llm_adjustment_valid'] = merged['llm_adjustment_valid'].fillna(False)\n",
    "\n",
    "merged.to_parquet('artifacts/results_stream_with_llm.parquet', index=False)\n",
    "print(\"WROTE: artifacts/results_stream_with_llm.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13684fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 5000000\n",
      "rows changed: 3857005 fraction: 0.771401\n",
      "top positive deltas:\n",
      " transaction_id  baseline_score  anomaly_score  llm_adjustment  final_score_x decision  final_score_y  delta\n",
      "      T5099999        0.043547       0.028381             0.0       0.043547    allow       0.043547    0.0\n",
      "      T3736945        0.043547       0.022447             0.0       0.043547    allow       0.043547    0.0\n",
      "      T3736899        0.043547       0.006114             0.0       0.043547    allow       0.043547    0.0\n",
      "      T3736907        0.043547       0.019794             0.0       0.043547    allow       0.043547    0.0\n",
      "      T1566711        0.043547       0.013271             0.0       0.043547    allow       0.043547    0.0\n",
      "      T3736910        0.045162       0.000000             0.0       0.045162    allow       0.045162    0.0\n",
      "      T3736912        0.043547       0.017218             0.0       0.043547    allow       0.043547    0.0\n",
      "      T1566703        0.043547       0.017163             0.0       0.043547    allow       0.043547    0.0\n",
      "      T1566699        0.043547       0.031506             0.0       0.043547    allow       0.043547    0.0\n",
      "      T1566697        0.043547       0.005666             0.0       0.043547    allow       0.043547    0.0\n",
      "top negative deltas:\n",
      " transaction_id  baseline_score  anomaly_score  llm_adjustment  final_score_x decision  final_score_y  delta\n",
      "       T612776             0.0            1.0             0.0            1.0    block            0.0   -1.0\n",
      "       T115628             0.0            1.0             0.0            1.0    block            0.0   -1.0\n",
      "       T619809             0.0            1.0             0.0            1.0    block            0.0   -1.0\n",
      "       T619796             0.0            1.0             0.0            1.0    block            0.0   -1.0\n",
      "       T314497             0.0            1.0             0.0            1.0    block            0.0   -1.0\n",
      "      T2252076             0.0            1.0             0.0            1.0    block            0.0   -1.0\n",
      "      T1218176             0.0            1.0             0.0            1.0    block            0.0   -1.0\n",
      "      T1218222             0.0            1.0             0.0            1.0    block            0.0   -1.0\n",
      "      T3043399             0.0            1.0             0.0            1.0    block            0.0   -1.0\n",
      "       T619650             0.0            1.0             0.0            1.0    block            0.0   -1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "before = pd.read_parquet('artifacts/results_stream.parquet')\n",
    "after = pd.read_parquet('artifacts/results_stream_recombined_with_llm.parquet')\n",
    "df = before.merge(after[['transaction_id','final_score']], on='transaction_id', suffixes=('_x', '_y'))\n",
    "df['delta'] = df['final_score_y'] - df['final_score_x']\n",
    "print(\"rows:\", len(df))\n",
    "print(\"rows changed:\", int((df['delta']!=0).sum()), \"fraction:\", float((df['delta']!=0).mean()))\n",
    "print(\"top positive deltas:\\n\", df.sort_values('delta',ascending=False).head(10).to_string(index=False))\n",
    "print(\"top negative deltas:\\n\", df.sort_values('delta').head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f94c091a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before columns: ['transaction_id', 'baseline_score', 'anomaly_score', 'llm_adjustment', 'final_score', 'decision']\n",
      "after columns: ['transaction_id', 'baseline_score', 'anomaly_score', 'llm_adjustment', 'final_score', 'decision', 'llm_adjustment_clamped', 'llm_adjustment_raw', 'llm_adjustment_valid', 'llm_evidence_ids', 'needs_review', 'topk_evidence_ids']\n",
      "total rows (merged): 5000000\n",
      "rows with final != baseline: 0 fraction: 0.0\n",
      "delta stats:\n",
      " count    5000000.0\n",
      "mean           0.0\n",
      "std            0.0\n",
      "min            0.0\n",
      "25%            0.0\n",
      "50%            0.0\n",
      "75%            0.0\n",
      "max            0.0\n",
      "\n",
      "Top positive deltas (10):\n",
      "transaction_id  baseline_score  final_score  anomaly_score  llm_adjustment_clamped  llm_adjustment_valid  delta\n",
      "       T100000        0.000000     0.000000       0.920105                     0.0                  True    0.0\n",
      "      T3433331        0.043547     0.043547       0.935135                     0.0                  True    0.0\n",
      "      T3433338        0.043547     0.043547       0.928039                     0.0                  True    0.0\n",
      "      T3433337        0.043547     0.043547       0.919305                     0.0                  True    0.0\n",
      "      T3433336        0.043547     0.043547       0.039023                     0.0                  True    0.0\n",
      "      T3433335        0.043547     0.043547       0.005214                     0.0                  True    0.0\n",
      "      T3433334        0.043547     0.043547       0.927511                     0.0                  True    0.0\n",
      "      T3433333        0.043547     0.043547       0.944052                     0.0                  True    0.0\n",
      "      T3433332        0.043547     0.043547       0.941184                     0.0                  True    0.0\n",
      "      T3433330        0.043547     0.043547       0.946119                     0.0                  True    0.0\n",
      "\n",
      "Top negative deltas (10):\n",
      "transaction_id  baseline_score  final_score  anomaly_score  llm_adjustment_clamped  llm_adjustment_valid  delta\n",
      "       T100000        0.000000     0.000000       0.920105                     0.0                  True    0.0\n",
      "      T3433337        0.043547     0.043547       0.919305                     0.0                  True    0.0\n",
      "      T3433336        0.043547     0.043547       0.039023                     0.0                  True    0.0\n",
      "      T3433335        0.043547     0.043547       0.005214                     0.0                  True    0.0\n",
      "      T3433334        0.043547     0.043547       0.927511                     0.0                  True    0.0\n",
      "      T3433333        0.043547     0.043547       0.944052                     0.0                  True    0.0\n",
      "      T3433332        0.043547     0.043547       0.941184                     0.0                  True    0.0\n",
      "      T3433331        0.043547     0.043547       0.935135                     0.0                  True    0.0\n",
      "      T3433330        0.043547     0.043547       0.946119                     0.0                  True    0.0\n",
      "      T3433329        0.043547     0.043547       0.935525                     0.0                  True    0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "before = pd.read_parquet('artifacts/results_stream.parquet')  # original baseline file\n",
    "after  = pd.read_parquet('artifacts/results_stream_recombined_with_llm.parquet')  # recombined output you produced\n",
    "\n",
    "print(\"before columns:\", before.columns.tolist()[:50])\n",
    "print(\"after columns:\", after.columns.tolist()[:50])\n",
    "\n",
    "# Ensure transaction_id as str\n",
    "before['transaction_id'] = before['transaction_id'].astype(str)\n",
    "after['transaction_id']  = after['transaction_id'].astype(str)\n",
    "\n",
    "# Merge to get baseline and final in one df\n",
    "df = before[['transaction_id','baseline_score']].merge(after[['transaction_id','final_score','anomaly_score','llm_adjustment_clamped','llm_adjustment_valid']], on='transaction_id', how='left')\n",
    "\n",
    "# Safety: fill missing columns if they don't exist\n",
    "for c in ('final_score','anomaly_score','llm_adjustment_clamped','llm_adjustment_valid'):\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "# Compute delta\n",
    "df['delta'] = df['final_score'].fillna(0.0) - df['baseline_score'].fillna(0.0)\n",
    "\n",
    "print(\"total rows (merged):\", len(df))\n",
    "print(\"rows with final != baseline:\", int((df['delta'] != 0).sum()), \"fraction:\", float((df['delta'] != 0).mean()))\n",
    "print(\"delta stats:\\n\", df['delta'].describe().to_string())\n",
    "# show extremes\n",
    "print(\"\\nTop positive deltas (10):\")\n",
    "print(df.sort_values('delta', ascending=False).head(10).to_string(index=False))\n",
    "print(\"\\nTop negative deltas (10):\")\n",
    "print(df.sort_values('delta').head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cddbebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 5000000\n",
      "total changed: 0 (0.000%)\n",
      "changed attributable to LLM only: 0 (0.000%)\n",
      "changed where anomaly influenced final (may include LLM too): 0 (0.000%)\n",
      "changed where both LLM nonzero and anomaly nonzero: 0 (0.000%)\n",
      "\n",
      "Sample changed_by_llm_only (up to 10):\n",
      "Empty DataFrame\n",
      "Columns: [transaction_id, baseline_score, llm_adjustment_clamped, intermediate_llm, final_score]\n",
      "Index: []\n",
      "\n",
      "Sample changed_by_anom (up to 10):\n",
      "Empty DataFrame\n",
      "Columns: [transaction_id, baseline_score, anomaly_score, llm_adjustment_clamped, intermediate_llm, final_score]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "before = pd.read_parquet('artifacts/results_stream.parquet')\n",
    "after  = pd.read_parquet('artifacts/results_stream_recombined_with_llm.parquet')\n",
    "\n",
    "before['transaction_id'] = before['transaction_id'].astype(str)\n",
    "after['transaction_id']  = after['transaction_id'].astype(str)\n",
    "\n",
    "# merge necessary cols\n",
    "cols_after = ['transaction_id','final_score','anomaly_score','llm_adjustment_clamped','llm_adjustment_valid']\n",
    "for c in cols_after[1:]:\n",
    "    if c not in after.columns:\n",
    "        after[c] = np.nan\n",
    "\n",
    "df = before[['transaction_id','baseline_score']].merge(after[cols_after], on='transaction_id', how='left').fillna(0.0)\n",
    "\n",
    "# intermediate after LLM (clamped to [0,1])\n",
    "df['intermediate_llm'] = (df['baseline_score'] + df['llm_adjustment_clamped']).clip(0.0,1.0)\n",
    "df['final_score'] = df['final_score'].clip(0.0,1.0)\n",
    "\n",
    "tol = 1e-8\n",
    "df['changed'] = (df['final_score'] - df['baseline_score']).abs() > tol\n",
    "df['changed_by_llm_only'] = df['changed'] & (df['llm_adjustment_clamped'].abs() > tol) & (df['final_score'].sub(df['intermediate_llm']).abs() <= tol)\n",
    "df['changed_by_anom'] = df['changed'] & (df['final_score'].sub(df['intermediate_llm']).abs() > tol)\n",
    "df['llm_nonzero'] = df['llm_adjustment_clamped'].abs() > tol\n",
    "df['anom_nonzero'] = df['anomaly_score'].abs() > tol\n",
    "\n",
    "total = len(df)\n",
    "n_changed = int(df['changed'].sum())\n",
    "n_llm_only = int(df['changed_by_llm_only'].sum())\n",
    "n_anom = int(df['changed_by_anom'].sum())\n",
    "n_both = int(((df['llm_nonzero']) & (df['anom_nonzero']) & df['changed']).sum())\n",
    "\n",
    "print(\"total rows:\", total)\n",
    "print(\"total changed:\", n_changed, f\"({n_changed/total:.3%})\")\n",
    "print(\"changed attributable to LLM only:\", n_llm_only, f\"({n_llm_only/total:.3%})\")\n",
    "print(\"changed where anomaly influenced final (may include LLM too):\", n_anom, f\"({n_anom/total:.3%})\")\n",
    "print(\"changed where both LLM nonzero and anomaly nonzero:\", n_both, f\"({n_both/total:.3%})\")\n",
    "\n",
    "# Sample rows for each bucket\n",
    "print(\"\\nSample changed_by_llm_only (up to 10):\")\n",
    "print(df[df['changed_by_llm_only']].head(10)[['transaction_id','baseline_score','llm_adjustment_clamped','intermediate_llm','final_score']].to_string(index=False))\n",
    "\n",
    "print(\"\\nSample changed_by_anom (up to 10):\")\n",
    "print(df[df['changed_by_anom']].head(10)[['transaction_id','baseline_score','anomaly_score','llm_adjustment_clamped','intermediate_llm','final_score']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "507268da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 1000\n",
      "llm_adjustment_valid: 0\n",
      "needs_review: 1000\n",
      "non-zero clamped: 0\n",
      "count    1000.0\n",
      "mean        0.0\n",
      "std         0.0\n",
      "min         0.0\n",
      "25%         0.0\n",
      "50%         0.0\n",
      "75%         0.0\n",
      "max         0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "p='artifacts/level3_llm/llm_parsed.parquet'\n",
    "df=pd.read_parquet(p)\n",
    "print(\"rows:\", len(df))\n",
    "print(\"llm_adjustment_valid:\", int(df['llm_adjustment_valid'].sum()))\n",
    "print(\"needs_review:\", int(df['needs_review'].sum()))\n",
    "print(\"non-zero clamped:\", int((df['llm_adjustment_clamped']!=0).sum()))\n",
    "print(df['llm_adjustment_clamped'].describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "619806d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID examples:\n",
      "Empty DataFrame\n",
      "Columns: [transaction_id, baseline_score, llm_adjustment_raw, llm_adjustment_clamped, llm_evidence_ids, explanation]\n",
      "Index: []\n",
      "\n",
      "NEEDS_REVIEW examples (show raw output):\n",
      "TX: T100000\n",
      "PROMPT (truncated): \n",
      "You are a strict fraud-triage assistant. Answer ONLY with a single valid JSON object that matches the schema exactly.\n",
      "\n",
      "Schema:\n",
      "{\n",
      "  \"transaction_id\": \"<string>\",\n",
      "  \"llm_adjustment\": <number>,       # additive delta to baseline score (range -0.10 .. +0.10). Will be clamped by caller to +/-0.0500\n",
      "  \"evidence_ids\": [\"E1\",\"E2\"],     # IDs referencing provided evidence items (must be subset of provided\n",
      "RAW_OUTPUT (truncated): __OLLAMA_ERROR__ RuntimeError('ollama generate failed: rc=1, stderr=Error: unknown command \"generate\" for \"ollama\"\\n')\n",
      "----\n",
      "TX: T100001\n",
      "PROMPT (truncated): \n",
      "You are a strict fraud-triage assistant. Answer ONLY with a single valid JSON object that matches the schema exactly.\n",
      "\n",
      "Schema:\n",
      "{\n",
      "  \"transaction_id\": \"<string>\",\n",
      "  \"llm_adjustment\": <number>,       # additive delta to baseline score (range -0.10 .. +0.10). Will be clamped by caller to +/-0.0500\n",
      "  \"evidence_ids\": [\"E1\",\"E2\"],     # IDs referencing provided evidence items (must be subset of provided\n",
      "RAW_OUTPUT (truncated): __OLLAMA_ERROR__ RuntimeError('ollama generate failed: rc=1, stderr=Error: unknown command \"generate\" for \"ollama\"\\n')\n",
      "----\n",
      "TX: T100002\n",
      "PROMPT (truncated): \n",
      "You are a strict fraud-triage assistant. Answer ONLY with a single valid JSON object that matches the schema exactly.\n",
      "\n",
      "Schema:\n",
      "{\n",
      "  \"transaction_id\": \"<string>\",\n",
      "  \"llm_adjustment\": <number>,       # additive delta to baseline score (range -0.10 .. +0.10). Will be clamped by caller to +/-0.0500\n",
      "  \"evidence_ids\": [\"E1\",\"E2\"],     # IDs referencing provided evidence items (must be subset of provided\n",
      "RAW_OUTPUT (truncated): __OLLAMA_ERROR__ RuntimeError('ollama generate failed: rc=1, stderr=Error: unknown command \"generate\" for \"ollama\"\\n')\n",
      "----\n",
      "TX: T100003\n",
      "PROMPT (truncated): \n",
      "You are a strict fraud-triage assistant. Answer ONLY with a single valid JSON object that matches the schema exactly.\n",
      "\n",
      "Schema:\n",
      "{\n",
      "  \"transaction_id\": \"<string>\",\n",
      "  \"llm_adjustment\": <number>,       # additive delta to baseline score (range -0.10 .. +0.10). Will be clamped by caller to +/-0.0500\n",
      "  \"evidence_ids\": [\"E1\",\"E2\"],     # IDs referencing provided evidence items (must be subset of provided\n",
      "RAW_OUTPUT (truncated): __OLLAMA_ERROR__ RuntimeError('ollama generate failed: rc=1, stderr=Error: unknown command \"generate\" for \"ollama\"\\n')\n",
      "----\n",
      "TX: T100004\n",
      "PROMPT (truncated): \n",
      "You are a strict fraud-triage assistant. Answer ONLY with a single valid JSON object that matches the schema exactly.\n",
      "\n",
      "Schema:\n",
      "{\n",
      "  \"transaction_id\": \"<string>\",\n",
      "  \"llm_adjustment\": <number>,       # additive delta to baseline score (range -0.10 .. +0.10). Will be clamped by caller to +/-0.0500\n",
      "  \"evidence_ids\": [\"E1\",\"E2\"],     # IDs referencing provided evidence items (must be subset of provided\n",
      "RAW_OUTPUT (truncated): __OLLAMA_ERROR__ RuntimeError('ollama generate failed: rc=1, stderr=Error: unknown command \"generate\" for \"ollama\"\\n')\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, json\n",
    "p='artifacts/level3_llm/llm_parsed.parquet'\n",
    "df=pd.read_parquet(p)\n",
    "print(\"VALID examples:\")\n",
    "print(df[df['llm_adjustment_valid']==True].head(5)[['transaction_id','baseline_score','llm_adjustment_raw','llm_adjustment_clamped','llm_evidence_ids','explanation']].to_string(index=False))\n",
    "print(\"\\nNEEDS_REVIEW examples (show raw output):\")\n",
    "nr = df[df['needs_review']==True].head(5)\n",
    "for tx in nr['transaction_id'].tolist():\n",
    "    # find raw entry\n",
    "    with open('artifacts/level3_llm/llm_raw.jsonl','r',encoding='utf-8') as fh:\n",
    "        for line in fh:\n",
    "            j=json.loads(line)\n",
    "            if j.get('transaction_id')==tx:\n",
    "                print(\"TX:\",tx)\n",
    "                print(\"PROMPT (truncated):\", j.get('prompt','')[:400])\n",
    "                print(\"RAW_OUTPUT (truncated):\", j.get('raw_output','')[:400])\n",
    "                print(\"----\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9736500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"transaction_id\": \"T100000\",\n",
      "  \"timestamp_utc\": \"2026-01-24T15:03:13.196882Z\",\n",
      "  \"prompt\": \"\\nYou are a strict fraud-triage assistant. Answer ONLY with a single valid JSON object that matches the schema exactly.\\n\\nSchema:\\n{\\n  \\\"transaction_id\\\": \\\"<string>\\\",\\n  \\\"llm_adjustment\\\": <number>,       # additive delta to baseline score (range -0.10 .. +0.10). Will be clamped by caller to +/-0.0500\\n  \\\"evidence_ids\\\": [\\\"E1\\\",\\\"E2\\\"],     # IDs referencing provided evidence items (must be subset of provided top_k_evidence ids)\\n  \\\"explanation\\\": \\\"<string>\\\",        # concise factual explanation based ONLY on provided evidence (<=250 chars)\\n  \\\"confidence\\\": <number>           # optional 0..1 numeric confidence\\n}\\n\\nRules:\\n- DO NOT output any text outside the JSON object (no markdown, no comments).\\n- Only reference evidence by id. evidence_ids MUST be a subset of the provided top_k_evidence ids.\\n- llm_adjustment must be a numeric additive delta; we will clamp it to +/- the configured max_delta.\\n- explanation must be concise and based solely on the evidence.\\n- Temperature must be 0 (deterministic).\\n- If you cannot produce a valid JSON that follows these rules, output the most faithful JSON indicating the reason, but keep the JSON schema shape.\\n\\nInput:\\n{\\n  \\\"transaction_id\\\": \\\"T100000\\\",\\n  \\\"baseline_score\\\": 0.0,\\n  \\\"features\\\": {\\n    \\\"anomaly_score\\\": 0.9201052578982496,\\n    \\\"llm_adjustment\\\": 0.0,\\n    \\\"final_score\\\": 0.9201052578982496,\\n    \\\"decision\\\": \\\"block\\\"\\n  },\\n  \\\"top_k_evidence\\\": [\\n    {\\n      \\\"id\\\": \\\"E1\\\",\\n      \\\"text\\\": \\\"baseline=0.0000\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"E2\\\",\\n      \\\"text\\\": \\\"merchant_risk=?\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"E3\\\",\\n      \\\"text\\\": \\\"recent device mismatch\\\"\\n    }\\n  ],\\n  \\\"max_delta\\\": 0.05\\n}\\n\\nRespond with the JSON object only.\\n\",\n",
      "  \"raw_output\": \"__NO_RUNNER__\",\n",
      "  \"model\": \"llama2\",\n",
      "  \"elapsed_seconds\": 7.152557373046875e-07\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "line = open('artifacts/level3_llm_test/llm_raw.jsonl','r',encoding='utf-8').readline()\n",
    "print(json.dumps(json.loads(line), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae64e916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{\n",
      "  \"transaction_id\": \"T100000\",\n",
      "  \"timestamp_utc\": \"2026-01-24T15:03:13.196882Z\",\n",
      "  \"prompt\": \"\\nYou are a strict fraud-triage assistant. Answer ONLY with a single valid JSON object that matches the schema exactly.\\n\\nSchema:\\n{\\n  \\\"transaction_id\\\": \\\"<string>\\\",\\n  \\\"llm_adjustment\\\": <number>,       # additive delta to baseline score (range -0.10 .. +0.10). Will be clamped by caller to +/-0.0500\\n  \\\"evidence_ids\\\": [\\\"E1\\\",\\\"E2\\\"],     # IDs referencing provided evidence items (must be subset of provided top_k_evidence ids)\\n  \\\"explanation\\\": \\\"<string>\\\",        # concise factual explanation based ONLY on provided evidence (<=250 chars)\\n  \\\"confidence\\\": <number>           # optional 0..1 numeric confidence\\n}\\n\\nRules:\\n- DO NOT output any text outside the JSON object (no markdown, no comments).\\n- Only reference evidence by id. evidence_ids MUST be a subset of the provided top_k_evidence ids.\\n- llm_adjustment must be a numeric additive delta; we will clamp it to +/- the configured max_delta.\\n- explanation must be concise and based solely on the evidence.\\n- Temperature must be 0 (deterministic).\\n- If you cannot produce a valid JSON that follows these rules, output the most faithful JSON indicating the reason, but keep the JSON schema shape.\\n\\nInput:\\n{\\n  \\\"transaction_id\\\": \\\"T100000\\\",\\n  \\\"baseline_score\\\": 0.0,\\n  \\\"features\\\": {\\n    \\\"anomaly_score\\\": 0.9201052578982496,\\n    \\\"llm_adjustment\\\": 0.0,\\n    \\\"final_score\\\": 0.9201052578982496,\\n    \\\"decision\\\": \\\"block\\\"\\n  },\\n  \\\"top_k_evidence\\\": [\\n    {\\n      \\\"id\\\": \\\"E1\\\",\\n      \\\"text\\\": \\\"baseline=0.0000\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"E2\\\",\\n      \\\"text\\\": \\\"merchant_risk=?\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"E3\\\",\\n      \\\"text\\\": \\\"recent device mismatch\\\"\\n    }\\n  ],\\n  \\\"max_delta\\\": 0.05\\n}\\n\\nRespond with the JSON object only.\\n\",\n",
      "  \"raw_output\": \"__NO_RUNNER__\",\n",
      "  \"model\": \"llama2\",\n",
      "  \"elapsed_seconds\": 7.152557373046875e-07\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib\n",
    "p=pathlib.Path('artifacts/level3_llm_test/llm_raw.jsonl')\n",
    "print(p.exists())\n",
    "if p.exists():\n",
    "    print(json.dumps(json.loads(p.read_text().splitlines()[0]), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
